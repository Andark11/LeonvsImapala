# ü¶Å Le√≥n vs Impala - Q-Learning

Sistema de aprendizaje por refuerzo donde un le√≥n aprende a cazar un impala mediante **Q-Learning**. El le√≥n no tiene estrategia programada, sino que aprende por experiencia tras miles de cacer√≠as simuladas.

[![Python3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License:MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## üéØ Caracter√≠sticas

- ‚úÖ **Q-Learning** con exploraci√≥n epsilon-greedy
- ‚úÖ **Visualizaci√≥n ASCII** en terminal (Grid 19√ó19)
- ‚úÖ **Coordenadas polares** para movimiento natural
- ‚úÖ **Persistencia** de modelos entrenados
- ‚úÖ **Tests unitarios** completos (9/9 pasando)

## üöÄ Instalaci√≥n y Setup

### Requerimientos

- **Python**: 3.8 o superior
- **Sistema Operativo**: Linux, macOS, Windows
- **Dependencias**: Ninguna (solo biblioteca est√°ndar de Python)

### Pasos de Instalaci√≥n

1. **Clonar el repositorio**
   \`\`\`bash
   git clone https://github.com/Andark11/LeonvsImapala.git
   cd LeonvsImapala
   \`\`\`

2. **Verificar versi√≥n de Python**
   \`\`\`bash
   python --version
   # Debe mostrar Python 3.8 o superior
   \`\`\`

3. **Ejecutar el sistema**
   \`\`\`bash
   python main.py
   \`\`\`

4. **Ejecutar tests (opcional)**
   \`\`\`bash
   python tests/test_basico.py
   # Resultado esperado: 9/9 tests pasando ‚úì
   \`\`\`

## üìñ Uso

### 1. Entrenar un Le√≥n

\`\`\`bash
python main.py
# Seleccionar: 1 (Sistema de Entrenamiento)
# Episodios recomendados: 100,000
# Guardar modelo con nombre descriptivo
\`\`\`

**Resultados t√≠picos:**
- 10,000 episodios ‚Üí 6-8% √©xito
- 100,000 episodios ‚Üí 10-12% √©xito

### 2. Visualizar Cacer√≠a

\`\`\`bash
python main.py
# Seleccionar: 2 (Simulaci√≥n Visual)
# Elegir modelo entrenado
# Ver cacer√≠a en grid 19√ó19
\`\`\`

### 3. Ejecutar Tests

\`\`\`bash
python tests/test_basico.py
# Resultado: 9/9 tests pasando ‚úì
\`\`\`

## üß† Q-Learning Explicado

### ¬øQu√© es el Modelo Q-Learning?

**Q-Learning** es un algoritmo de **Aprendizaje por Refuerzo** (Reinforcement Learning) que permite a un agente (el le√≥n) aprender la mejor acci√≥n a tomar en cada situaci√≥n sin necesidad de un modelo expl√≠cito del entorno.

#### Concepto del Modelo

El modelo consiste en una **Tabla Q** que almacena valores Q(s,a) para cada combinaci√≥n de:
- **Estado (s)**: Situaci√≥n actual del mundo (posici√≥n le√≥n, distancia impala, visibilidad, etc.)
- **Acci√≥n (a)**: Movimiento posible (avanzar, esconderse, atacar)

**Valor Q(s,a)**: Representa "qu√© tan bueno" es tomar la acci√≥n `a` en el estado `s`. Un valor alto indica que hist√≥ricamente esa acci√≥n ha llevado a buenos resultados.

#### Tabla Q - Estructura

\`\`\`python
# Ejemplo de Tabla Q despu√©s de 1000 episodios
Q = {
    Estado(pos=1, dist=9.5, impala_bebe=True, escondido=False): {
        'avanzar': 45.2,      # Buena opci√≥n
        'esconderse': 58.7,   # Mejor opci√≥n (valor m√°s alto)
        'atacar': -15.3       # Mala opci√≥n (demasiado lejos)
    },
    Estado(pos=1, dist=2.0, impala_bebe=True, escondido=True): {
        'avanzar': 35.8,
        'esconderse': 12.1,
        'atacar': 78.5        # Mejor opci√≥n (cerca y escondido)
    }
}
\`\`\`

### La Ecuaci√≥n de Bellman

El le√≥n aprende actualizando los valores Q mediante la **ecuaci√≥n de Bellman**:

\`\`\`
Q(s,a) -> Q(s,a) + Œ±[r + Œ≥¬∑max Q(s',a') - Q(s,a)]
\`\`\`

**Desglose de la ecuaci√≥n:**

1. **Q(s,a)**: Valor Q actual que queremos actualizar
2. **Œ± (alpha)** = 0.05: **Tasa de aprendizaje**
   - Controla qu√© tan r√°pido se actualizan los valores
   - Valor bajo (0.05) = aprendizaje gradual y estable
   - Evita cambios bruscos por experiencias aisladas

3. **r**: **Recompensa inmediata** obtenida
   - +100 puntos: Cacer√≠a exitosa (captur√≥ al impala)
   - -50 puntos: Fracaso (impala escap√≥)
   - +1 punto: Por cada cuadro que se acerc√≥
   - -10 puntos: Si el impala detecta al le√≥n prematuramente

4. **Œ≥ (gamma)** = 0.9: **Factor de descuento**
   - Importancia de recompensas futuras vs inmediatas
   - 0.9 = valora mucho las consecuencias a largo plazo
   - M√°s cercano a 1 = m√°s "visionario"

5. **max Q(s',a')**: **Mejor valor Q futuro**
   - Mejor acci√≥n posible en el nuevo estado s'
   - Estimaci√≥n del valor futuro √≥ptimo
   - Gu√≠a hacia decisiones que maximizan recompensa total

6. **[r + Œ≥¬∑max Q(s',a') - Q(s,a)]**: **Error de predicci√≥n**
   - Diferencia entre lo esperado y lo obtenido
   - Si es positivo: la acci√≥n fue mejor de lo esperado
   - Si es negativo: fue peor de lo esperado

#### Interpretaci√≥n Intuitiva

La ecuaci√≥n dice: *"El valor de tomar la acci√≥n A en el estado S es mi estimaci√≥n actual m√°s un ajuste basado en lo que realmente pas√≥ (recompensa inmediata + mejor futuro posible)"*

### Ejemplo de Aprendizaje

**Episodio 1** (sin experiencia):
\`\`\`
Estado: Le√≥n en pos 1, distancia 9.5, impala bebiendo
Q inicial: Q(estado, atacar) = 0

Le√≥n toma acci√≥n: ATACAR (aleatorio, no sabe que es malo)
Resultado: Impala detecta el sonido y escapa
Recompensa: r = -50

Actualizaci√≥n:
Q(estado, atacar) = 0 + 0.05[-50 + 0 - 0]
Q(estado, atacar) = -2.5

üß† Aprendi√≥: "Atacar desde lejos es muy mala idea"
\`\`\`

**Episodio 100** (con experiencia):
\`\`\`
Mismo estado: Le√≥n en pos 1, distancia 9.5, impala bebiendo
Q actual: Q(estado, esconderse) = 45.0 (mejor opci√≥n conocida)

Le√≥n toma acci√≥n: ESCONDERSE (explota conocimiento)
Resultado: No es detectado, puede acercarse despu√©s
Recompensa: r = +5 (bono por estrategia)

Actualizaci√≥n:
Q(estado, esconderse) = 45 + 0.05[5 + 0.9(50) - 45]
Q(estado, esconderse) = 45 + 0.05[5 + 45 - 45]
Q(estado, esconderse) = 45.25

üß† Reforz√≥: "Esconderse desde lejos funciona bien"
\`\`\`

### Proceso de Aprendizaje

1. **Exploraci√≥n** ‚Üí Prueba acciones aleatorias para descubrir
2. **Experiencia** ‚Üí Acumula resultados (estado ‚Üí acci√≥n ‚Üí recompensa ‚Üí nuevo estado)
3. **Actualizaci√≥n** ‚Üí Mejora valores Q con la ecuaci√≥n de Bellman
4. **Explotaci√≥n** ‚Üí Usa conocimiento aprendido (elige acciones con Q alto)
5. **Convergencia** ‚Üí Despu√©s de miles de episodios, desarrolla estrategia √≥ptima

### Pol√≠tica Epsilon-Greedy

Balancea **exploraci√≥n** (descubrir) vs **explotaci√≥n** (usar lo aprendido):

\`\`\`python
if random() < epsilon:
    acci√≥n = aleatoria()      # EXPLORAR: probar algo nuevo
else:
    acci√≥n = argmax(Q[estado])  # EXPLOTAR: mejor acci√≥n conocida
\`\`\`

**Decaimiento de epsilon:**
- Inicio: Œµ = 1.0 (100% exploraci√≥n - el le√≥n no sabe nada)
- Decremento: Œµ -= 0.9/episodios_totales (decrece gradualmente)
- Final: Œµ = 0.1 (90% explotaci√≥n, 10% exploraci√≥n - el le√≥n usa su experiencia pero sigue probando cosas nuevas ocasionalmente)

## üåç Coordenadas Polares

### ¬øQu√© son las Coordenadas Polares?

En lugar de usar coordenadas cartesianas (x, y), las coordenadas polares definen un punto mediante:
- **r (radio)**: Distancia desde el centro (el abrevadero)
- **Œ∏ (theta)**: √Ångulo desde el norte (0¬∞ = Norte, aumenta en sentido horario)

### Diagrama del Sistema

\`\`\`
        N (0¬∞)
         |
    8    1    2
     \   |   /
315¬∞ \  0¬∞  / 45¬∞
      \ | /
  W -- AB -- E
      / | \
270¬∞ /  |  \ 90¬∞
    /   |   \
    7   6   5
        |
       S (180¬∞)
        
Posiciones:
1 = Norte (N)      - 0¬∞
2 = Noreste (NE)   - 45¬∞
3 = Este (E)       - 90¬∞
4 = Sureste (SE)   - 135¬∞
5 = Sur (S)        - 180¬∞
6 = Suroeste (SO)  - 225¬∞
7 = Oeste (O)      - 270¬∞
8 = Noroeste (NO)  - 315¬∞
AB = Abrevadero    - Centro (0, 0)
\`\`\`

### C√°lculo de √Ångulo desde Posici√≥n

**F√≥rmula:**
\`\`\`python
Œ∏ = (posici√≥n - 1) √ó 45¬∞
\`\`\`

**Ejemplos:**
\`\`\`
posici√≥n 1 (Norte):     Œ∏ = (1-1) √ó 45¬∞ = 0¬∞
posici√≥n 2 (Noreste):   Œ∏ = (2-1) √ó 45¬∞ = 45¬∞
posici√≥n 3 (Este):      Œ∏ = (3-1) √ó 45¬∞ = 90¬∞
posici√≥n 5 (Sur):       Œ∏ = (5-1) √ó 45¬∞ = 180¬∞
posici√≥n 7 (Oeste):     Œ∏ = (7-1) √ó 45¬∞ = 270¬∞
posici√≥n 8 (Noroeste):  Œ∏ = (8-1) √ó 45¬∞ = 315¬∞
\`\`\`

### Conversi√≥n Polar ‚Üí Cartesiano

Para convertir coordenadas polares (r, Œ∏) a coordenadas cartesianas (x, y):

**F√≥rmulas:**
\`\`\`python
x = r √ó sin(Œ∏)
y = r √ó cos(Œ∏)
\`\`\`

**Ejemplos con RADIO = 9.5:**

#### Posici√≥n 1 (Norte, Œ∏=0¬∞):
\`\`\`python
x = 9.5 √ó sin(0¬∞) = 9.5 √ó 0 = 0.0
y = 9.5 √ó cos(0¬∞) = 9.5 √ó 1 = 9.5
‚Üí Coordenadas: (0.0, 9.5)
\`\`\`

#### Posici√≥n 2 (Noreste, Œ∏=45¬∞):
\`\`\`python
x = 9.5 √ó sin(45¬∞) = 9.5 √ó 0.707 = 6.72
y = 9.5 √ó cos(45¬∞) = 9.5 √ó 0.707 = 6.72
‚Üí Coordenadas: (6.72, 6.72)
\`\`\`

#### Posici√≥n 3 (Este, Œ∏=90¬∞):
\`\`\`python
x = 9.5 √ó sin(90¬∞) = 9.5 √ó 1 = 9.5
y = 9.5 √ó cos(90¬∞) = 9.5 √ó 0 = 0.0
‚Üí Coordenadas: (9.5, 0.0)
\`\`\`

#### Posici√≥n 5 (Sur, Œ∏=180¬∞):
\`\`\`python
x = 9.5 √ó sin(180¬∞) = 9.5 √ó 0 = 0.0
y = 9.5 √ó cos(180¬∞) = 9.5 √ó (-1) = -9.5
‚Üí Coordenadas: (0.0, -9.5)
\`\`\`

#### Posici√≥n 7 (Oeste, Œ∏=270¬∞):
\`\`\`python
x = 9.5 √ó sin(270¬∞) = 9.5 √ó (-1) = -9.5
y = 9.5 √ó cos(270¬∞) = 9.5 √ó 0 = 0.0
‚Üí Coordenadas: (-9.5, 0.0)
\`\`\`

### C√°lculo de Distancia

Para calcular la distancia entre dos puntos en coordenadas cartesianas:

**F√≥rmula de distancia euclidiana:**
\`\`\`python
d = ‚àö[(x‚ÇÇ - x‚ÇÅ)¬≤ + (y‚ÇÇ - y‚ÇÅ)¬≤]
\`\`\`

#### Ejemplo: Distancia desde Le√≥n (pos 1) hasta Abrevadero

\`\`\`python
# Le√≥n en posici√≥n 1 (Norte)
le√≥n_x = 0.0
le√≥n_y = 9.5

# Abrevadero en el centro
abrevadero_x = 0.0
abrevadero_y = 0.0

# Distancia
d = ‚àö[(0.0 - 0.0)¬≤ + (0.0 - 9.5)¬≤]
d = ‚àö[0 + 90.25]
d = ‚àö90.25
d = 9.5 unidades ‚úì
\`\`\`

#### Ejemplo: Distancia entre Le√≥n (pos 1) e Impala (pos 5)

\`\`\`python
# Le√≥n en posici√≥n 1 (Norte): (0.0, 9.5)
# Impala en posici√≥n 5 (Sur): (0.0, -9.5)

d = ‚àö[(0.0 - 0.0)¬≤ + (-9.5 - 9.5)¬≤]
d = ‚àö[0 + (-19)¬≤]
d = ‚àö361
d = 19.0 unidades (di√°metro completo)
\`\`\`

### Conversi√≥n a Grid ASCII

Para visualizaci√≥n en terminal, se convierte a un grid 19√ó19:

**F√≥rmula:**
\`\`\`python
ESCALA = 1.9
grid_x = int(x_cartesiano * ESCALA) + 9  # +9 para centrar (grid 0-18)
grid_y = int(y_cartesiano * ESCALA) + 9
\`\`\`

**Ejemplo - Le√≥n en posici√≥n 1:**
\`\`\`python
# Coordenadas cartesianas: (0.0, 9.5)
grid_x = int(0.0 √ó 1.9) + 9 = 0 + 9 = 9
grid_y = int(9.5 √ó 1.9) + 9 = 18 + 9 = 27 ‚Üí ajustado a 18 (l√≠mite grid)

# En el grid ASCII, el le√≥n aparece en columna 9, fila superior
\`\`\`

### Ventajas del Sistema Polar

- **Natural para escenario circular**: El abrevadero es el centro natural
- **Simplifica c√°lculos de distancia**: Solo necesitamos el radio
- **Movimiento intuitivo**: Avanzar = reducir r (acercarse al centro)
- **8 direcciones claras**: Posiciones cardinales f√°ciles de entender

### Par√°metros del Sistema

- **RADIO** = 9.5 unidades (distancia inicial le√≥n-impala desde el abrevadero)
- **ESCALA** = 1.9 (factor de conversi√≥n a grid ASCII 19√ó19)
- **Posiciones**: 8 puntos cardinales + 1 centro (abrevadero)
- **Rango √°ngulos**: 0¬∞ a 315¬∞ (incrementos de 45¬∞)

## ÔøΩ Base de Conocimientos

El sistema incluye una **base de conocimientos** que permite al le√≥n aplicar experiencias aprendidas a situaciones nuevas mediante **generalizaci√≥n**.

### ¬øQu√© es la Base de Conocimientos?

La base de conocimientos almacena patrones de comportamiento exitosos que el le√≥n ha aprendido durante el entrenamiento. Est√° implementada en el m√≥dulo `knowledge/base_conocimientos.py`.

**Estructura:**
\`\`\`python
{
    'situacion_tipo': {
        'condiciones': {
            'distancia_minima': 3.0,
            'distancia_maxima': 5.0,
            'impala_bebiendo': True,
            'leon_escondido': True
        },
        'accion_recomendada': 'atacar',
        'exitos': 145,
        'intentos': 200,
        'tasa_exito': 0.725
    }
}
\`\`\`

### Tipos de Conocimiento Almacenado

1. **Patrones de distancia**: Qu√© acci√≥n tomar seg√∫n la distancia al impala
   - Distancia > 7: Esconderse y avanzar
   - Distancia 3-7: Avanzar cautelosamente
   - Distancia < 3: Atacar si est√° escondido

2. **Patrones de visibilidad**: C√≥mo actuar seg√∫n si el impala puede verlo
   - Impala bebiendo ‚Üí Avanzar r√°pidamente
   - Impala mirando ‚Üí Esconderse primero

3. **Patrones de posici√≥n**: Mejores posiciones iniciales para cazar
   - Posiciones laterales (2, 4, 6, 8) tienen mayor √©xito
   - Posiciones cardinales (1, 3, 5, 7) requieren m√°s estrategia

### Generalizaci√≥n de Conocimientos

El m√≥dulo `knowledge/generalizacion.py` permite aplicar conocimiento aprendido a situaciones similares:

**Proceso:**
1. **Identificar situaci√≥n actual**: Extraer caracter√≠sticas del estado (distancia, visibilidad, etc.)
2. **Buscar patrones similares**: Encontrar situaciones conocidas con caracter√≠sticas parecidas
3. **Calcular similitud**: Medir qu√© tan parecida es la situaci√≥n actual a las conocidas
4. **Aplicar conocimiento**: Usar la acci√≥n que funcion√≥ en situaciones similares
5. **Actualizar base**: Si la acci√≥n funciona, reforzar el patr√≥n

**Ejemplo de generalizaci√≥n:**
\`\`\`python
# Situaci√≥n aprendida: distancia=4.2, impala_bebe=True, escondido=True ‚Üí atacar (√©xito)
# Situaci√≥n nueva:     distancia=4.5, impala_bebe=True, escondido=True
# Similitud: 95% ‚Üí Aplicar "atacar" con alta confianza
\`\`\`

### Ventajas de la Base de Conocimientos

- ‚úÖ **Aprendizaje m√°s r√°pido**: No necesita explorar todas las situaciones desde cero
- ‚úÖ **Mejor generalizaci√≥n**: Aplica experiencias previas a situaciones nuevas
- ‚úÖ **Conocimiento interpretable**: Humanos pueden entender qu√© aprendi√≥ el le√≥n
- ‚úÖ **Transferencia de conocimiento**: Puede compartirse entre diferentes modelos

## üéì Proceso de Entrenamiento

El entrenamiento del le√≥n sigue un ciclo de aprendizaje por refuerzo supervisado por Q-Learning.

### Fases del Entrenamiento

#### 1. Inicializaci√≥n
\`\`\`python
# Configuraci√≥n inicial
alpha = 0.05      # Tasa de aprendizaje (qu√© tan r√°pido aprende)
gamma = 0.9       # Factor de descuento (importancia del futuro)
epsilon = 1.0     # Exploraci√≥n inicial (100% aleatorio)
episodios = 100000
\`\`\`

#### 2. Ciclo de Episodios

**Para cada episodio de cacer√≠a:**

1. **Setup inicial**
   \`\`\`python
   - Posici√≥n aleatoria del le√≥n (1-8)
   - Impala en el abrevadero (centro)
   - Tabla Q cargada (si existe modelo previo)
   \`\`\`

2. **Loop de turnos** (m√°ximo 50 turnos por episodio)
   \`\`\`python
   while caceria_activa:
       # a) Observar estado actual
       estado = obtener_estado_mundo()
       
       # b) Decidir acci√≥n (epsilon-greedy)
       if random() < epsilon:
           accion = aleatoria()      # Explorar
       else:
           accion = mejor_Q(estado)  # Explotar
       
       # c) Ejecutar acci√≥n
       nuevo_estado, recompensa, terminado = ejecutar(accion)
       
       # d) Actualizar Q-Learning
       Q[estado][accion] += alpha * (
           recompensa + gamma * max(Q[nuevo_estado]) - Q[estado][accion]
       )
       
       # e) Verificar fin
       if terminado:
           break
   \`\`\`

3. **Registro de resultados**
   \`\`\`python
   - √âxito/fracaso de la cacer√≠a
   - Recompensa total acumulada
   - N√∫mero de turnos utilizados
   - Actualizaci√≥n de estad√≠sticas
   \`\`\`

4. **Decremento de epsilon**
   \`\`\`python
   epsilon = max(0.1, epsilon - (0.9 / episodios))
   # Reduce exploraci√≥n gradualmente
   # Episodio 1:     Œµ = 1.0   (100% exploraci√≥n)
   # Episodio 50000: Œµ ‚âà 0.55  (55% exploraci√≥n)
   # Episodio 100000: Œµ = 0.1   (10% exploraci√≥n)
   \`\`\`

#### 3. Guardado del Modelo

Cada cierto n√∫mero de episodios (ej: cada 10,000):
\`\`\`python
{
    "q_table": {...},           # Tabla Q completa
    "episodios": 100000,        # Episodios completados
    "exitos": 10245,           # Cacer√≠as exitosas
    "tasa_exito": 0.10245,     # 10.245% √©xito
    "epsilon_final": 0.1,      # Exploraci√≥n final
    "alpha": 0.05,             # Tasa de aprendizaje
    "gamma": 0.9               # Factor de descuento
}
\`\`\`

### Progresi√≥n T√≠pica del Entrenamiento

| Episodios | Tasa √âxito | Epsilon | Comportamiento |
|-----------|------------|---------|----------------|
| 0 - 10,000 | 2-4% | 1.0 ‚Üí 0.91 | Exploraci√≥n ca√≥tica, aprende b√°sicos |
| 10,000 - 30,000 | 4-7% | 0.91 ‚Üí 0.73 | Identifica patrones, mejora estrategia |
| 30,000 - 60,000 | 7-9% | 0.73 ‚Üí 0.46 | Consolida conocimiento, m√°s consistente |
| 60,000 - 100,000 | 9-12% | 0.46 ‚Üí 0.1 | Explota conocimiento, ajustes finos |

### Monitoreo del Entrenamiento

Durante el entrenamiento, el sistema muestra:
\`\`\`
Episodio 45000/100000 | √âxitos: 3402 | Tasa: 7.56% | Œµ: 0.595
√öltimos 1000: 78 √©xitos (7.8%)
Recompensa promedio: +12.4
\`\`\`

## üîÑ Proceso de Adquisici√≥n de Conocimientos

El le√≥n adquiere conocimientos mediante tres mecanismos complementarios:

### 1. Aprendizaje por Refuerzo (Q-Learning)

**Mecanismo principal** de adquisici√≥n de conocimientos:

\`\`\`
Experiencia ‚Üí Actualizaci√≥n Q ‚Üí Mejora de pol√≠tica ‚Üí Nueva experiencia
\`\`\`

**Proceso detallado:**
1. **Exploraci√≥n**: Prueba acciones en diferentes estados
2. **Recompensa**: Recibe retroalimentaci√≥n (+100 √©xito, -50 fracaso, +1 acercamiento)
3. **Actualizaci√≥n**: Ajusta valores Q seg√∫n ecuaci√≥n de Bellman
4. **Refinamiento**: Mejora estimaciones con cada experiencia

**Ejemplo de adquisici√≥n:**
\`\`\`
Episodio 1:
  Estado: (pos=3, dist=9.5, escondido=False)
  Acci√≥n: avanzar ‚Üí Impala detecta ‚Üí Huye
  Recompensa: -50
  Q[estado][avanzar] = 0 + 0.05(-50) = -2.5
  Conocimiento: "No avanzar visible desde lejos"

Episodio 500:
  Mismo estado
  Acci√≥n: esconderse ‚Üí Oculto ‚Üí Puede avanzar despu√©s
  Recompensa: +5
  Q[estado][esconderse] = 20 + 0.05(5 + 0.9(30) - 20) = 21.35
  Conocimiento: "Esconderse primero desde lejos es mejor"
\`\`\`

### 2. Generalizaci√≥n de Patrones

**Mecanismo secundario** que acelera el aprendizaje:

\`\`\`python
# El le√≥n identifica que situaciones similares requieren acciones similares
patron_identificado = {
    'caracteristicas': ['distancia_corta', 'impala_bebiendo', 'escondido'],
    'accion': 'atacar',
    'confianza': 0.85
}

# Aplica este patr√≥n a nuevas situaciones con caracter√≠sticas similares
\`\`\`

**Proceso:**
1. **Extracci√≥n de caracter√≠sticas**: Identifica atributos clave del estado
2. **Clustering**: Agrupa estados similares
3. **Pattern matching**: Encuentra patrones recurrentes
4. **Aplicaci√≥n**: Usa patrones exitosos en situaciones nuevas

### 3. Persistencia y Transferencia

**Mecanismo de memoria a largo plazo:**

\`\`\`python
# Guardar conocimiento
modelo = {
    'q_table': tabla_Q,              # Conocimiento espec√≠fico
    'patrones': patrones_exitosos,   # Conocimiento generalizado
    'estadisticas': metricas          # Rendimiento hist√≥rico
}
guardar_modelo("leon_experto.json", modelo)

# Cargar conocimiento
modelo_previo = cargar_modelo("leon_experto.json")
# El le√≥n contin√∫a aprendiendo desde donde qued√≥
\`\`\`

**Ventajas:**
- ‚úÖ No pierde conocimiento entre sesiones
- ‚úÖ Puede entrenar incremental (agregar m√°s episodios)
- ‚úÖ Permite comparar diferentes estrategias
- ‚úÖ Facilita transferencia de conocimiento

### M√©tricas de Conocimiento Adquirido

El sistema eval√∫a la calidad del conocimiento mediante:

1. **Tasa de √©xito**: % de cacer√≠as exitosas
   \`\`\`python
   tasa_exito = cacer√≠as_exitosas / total_cacer√≠as
   # Objetivo: > 10% (el impala tiene ventaja natural)
   \`\`\`

2. **Recompensa promedio**: Valor promedio obtenido por episodio
   \`\`\`python
   recompensa_promedio = suma_recompensas / total_episodios
   # Positivo = m√°s √©xitos que fracasos
   \`\`\`

3. **Convergencia**: Estabilizaci√≥n de valores Q
   \`\`\`python
   convergencia = desviacion_estandar(ultimos_1000_episodios)
   # Baja desviaci√≥n = conocimiento estable
   \`\`\`

4. **Cobertura de estados**: % de estados explorados
   \`\`\`python
   cobertura = estados_visitados / total_estados_posibles
   # Mayor cobertura = conocimiento m√°s completo
   \`\`\`

## ÔøΩüìÅ Estructura

\`\`\`
LeonvsImapala/
‚îú‚îÄ‚îÄ agents/          # Le√≥n e Impala
‚îú‚îÄ‚îÄ simulation/      # Motor de cacer√≠a
‚îú‚îÄ‚îÄ knowledge/       # Base de conocimientos
‚îú‚îÄ‚îÄ learning/        # Q-Learning y entrenamiento
‚îú‚îÄ‚îÄ storage/         # Persistencia JSON
‚îú‚îÄ‚îÄ ui/              # Interfaces (terminal + matplotlib)
‚îú‚îÄ‚îÄ tests/           # Tests unitarios
‚îú‚îÄ‚îÄ modelos/         # Modelos entrenados (generados)
‚îú‚îÄ‚îÄ environment.py   # Abrevadero y coordenadas
‚îî‚îÄ‚îÄ main.py          # Punto de entrada
\`\`\`

## üéÆ Reglas del Sistema

### Entorno
- **Grid:** 19√ó19 cuadros
- **RADIO:** 9.5 unidades (distancia inicial le√≥n-impala)
- **Captura:** Distancia ‚â§ 0.5 unidades

### Acciones del Le√≥n
| Acci√≥n | Velocidad | Descripci√≥n |
|--------|-----------|-------------|
| Avanzar | 1 cuadro/T | Acercarse sigilosamente |
| Esconderse | 0 cuadros/T | Ocultarse (invisible) |
| Atacar | 2 cuadros/T | Sprint final |

### Comportamiento del Impala
- **Visi√≥n:** Cono de 120¬∞ (puede rotar)
- **Huida:** Aceleraci√≥n progresiva (1‚Üí2‚Üí3‚Üí4... cuadros/T)
- **Condiciones de huida:**
  1. Ve al le√≥n (no escondido)
  2. Le√≥n ataca
  3. Distancia < 3 cuadros

## üß™ Tests Unitarios

\`\`\`bash
‚úì Abrevadero - Coordenadas (RADIO=9.5)
‚úì Abrevadero - Distancias
‚úì Le√≥n - Acciones (avanzar, esconderse, atacar)
‚úì Impala - Acciones (ver, beber, huir)
‚úì Base Conocimientos - Tabla Q
‚úì Q-Learning - Selecci√≥n epsilon-greedy
‚úì Sistema Recompensas
‚úì Cacer√≠a Completa - End-to-end
‚úì Cacer√≠a Turno a Turno
\`\`\`

**Cobertura:** Entorno, agentes, aprendizaje, simulaci√≥n, conocimiento

## üìä Resultados

### Modelo EM4 (100,000 episodios)
\`\`\`
Total de cacer√≠as: 100,000
Cacer√≠as exitosas: 10,450
Tasa de √©xito: 10.45%
Experiencias √∫nicas: 145,135
Tiempo: ~15 minutos
\`\`\`

### Progresi√≥n T√≠pica
| Episodios | Tasa √âxito |
|-----------|------------|
| 1,000 | 3-4% |
| 10,000 | 6-8% |
| 50,000 | 9-10% |
| 100,000 | 10-12% |

## üîß Tecnolog√≠a

- **Python 3.8+** con type hints
- **Q-Learning** (Reinforcement Learning)
- **Sin dependencias** (solo stdlib)
- **JSON** para persistencia
- **ASCII art** para visualizaci√≥n

## üêõ Troubleshooting

**Error: "El RADIO del modelo no coincide"**
‚Üí Re-entrenar con RADIO=9.5

**El impala siempre escapa**
‚Üí Normal con pocos episodios. Entrenar 100K+

**No se ven colores en terminal**
‚Üí Terminal no soporta ANSI, pero funciona igual

## üìö Referencias

- Watkins, C.J.C.H. (1989). *Learning from Delayed Rewards*
- Sutton & Barto (2018). *Reinforcement Learning: An Introduction*

## üìÑ Licencia

Sin licencia, todos los derechos reservados.

## üë®‚Äçüíª Autores

**Proyecto Final - Sistemas Inteligentes**  
Implementaci√≥n educativa de Q-Learning aplicado a caza predador-presa

**Equipo de desarrollo:**

- **Alvarado Mart√≠nez Miguel Eduardo**
- **Garc√≠a Retana Alba Sughey**
- **Soria Cabrera Andr√©s**
- **Sosa P√©rez Dariana Montserrat**
---

**Estado:** ‚úÖ Sistema completo y funcional  
**Versi√≥n:** 1.0.0  
**√öltima actualizaci√≥n:** Diciembre 2025
 