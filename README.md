# ğŸ¦ LeÃ³n vs Impala - Sistema de Aprendizaje por Refuerzo# LeÃ³n vs Impala - Sistema de Aprendizaje por Refuerzo



Sistema de aprendizaje automÃ¡tico donde un leÃ³n joven aprende a cazar un impala en un abrevadero mediante **Q-Learning**. El leÃ³n no tiene estrategia programada, sino que aprende por experiencia tras miles de cacerÃ­as simuladas.Sistema de aprendizaje automÃ¡tico donde un leÃ³n joven aprende a cazar un impala en un abrevadero mediante aprendizaje por refuerzo (Q-Learning).



## ğŸ¯ CaracterÃ­sticas Principales## ğŸš€ Inicio RÃ¡pido



- âœ… **Aprendizaje por Refuerzo**: Q-Learning con exploraciÃ³n epsilon-greedy```bash

- âœ… **VisualizaciÃ³n en Terminal**: Grid ASCII 19Ã—19 con colores ANSI# Ejecutar el programa principal

- âœ… **Persistencia de Modelos**: Guarda y carga conocimientos aprendidospython main.py

- âœ… **Sistema de GeneralizaciÃ³n**: Abstrae experiencias en patrones reutilables

- âœ… **MÃ©tricas de Rendimiento**: Seguimiento de tasa de Ã©xito durante entrenamiento# Ejecutar tests

- âœ… **SimulaciÃ³n Realista**: Coordenadas polares, lÃ­nea de visiÃ³n, fÃ­sica de movimientopython tests/test_basico.py

```

## ğŸš€ Inicio RÃ¡pido

## ğŸ“‹ Requisitos

### Requisitos

- Python 3.8 o superior- Python 3.8 o superior

- Sin dependencias externas (solo librerÃ­as estÃ¡ndar)- Solo librerÃ­as estÃ¡ndar (sin dependencias externas)



### InstalaciÃ³n## ğŸ“ Estructura del Proyecto



```bash```

# Clonar el repositorioLeonvsImapala/

cd /home/holly/Proyectos/LeonvsImapalaâ”œâ”€â”€ .github/

â”‚   â””â”€â”€ copilot-instructions.md    # Instrucciones para GitHub Copilot

# El proyecto estÃ¡ listo para ejecutarseâ”‚

python main.pyâ”œâ”€â”€ agents/                         # Agentes del sistema

```â”‚   â”œâ”€â”€ __init__.py

â”‚   â”œâ”€â”€ impala.py                  # Comportamiento del impala

### Uso BÃ¡sicoâ”‚   â””â”€â”€ leon.py                    # Comportamiento del leÃ³n

â”‚

```bashâ”œâ”€â”€ simulation/                     # Motor de simulaciÃ³n

python main.pyâ”‚   â”œâ”€â”€ __init__.py

```â”‚   â”œâ”€â”€ caceria.py                 # LÃ³gica de incursiÃ³n de cacerÃ­a

â”‚   â”œâ”€â”€ tiempo.py                  # GestiÃ³n de unidades de tiempo T

**MenÃº Principal:**â”‚   â””â”€â”€ verificador.py             # VerificaciÃ³n de condiciones

1. **Sistema de Entrenamiento** - Entrenar al leÃ³n (recomendado: 10,000-100,000 episodios)â”‚

2. **SimulaciÃ³n Visual** - Ver cacerÃ­as con visualizaciÃ³n ASCII en tiempo realâ”œâ”€â”€ knowledge/                      # âœ… Sistema de conocimiento

3. **Acerca del Proyecto** - InformaciÃ³n tÃ©cnica detalladaâ”‚   â”œâ”€â”€ __init__.py

4. **Salir**â”‚   â”œâ”€â”€ base_conocimientos.py     # Almacenamiento de experiencias

â”‚   â””â”€â”€ generalizacion.py         # AbstracciÃ³n de patrones

## ğŸ“ Estructura del Proyectoâ”‚

â”œâ”€â”€ learning/                       # âœ… Aprendizaje por refuerzo

```â”‚   â”œâ”€â”€ __init__.py

LeonvsImapala/â”‚   â”œâ”€â”€ q_learning.py             # Algoritmo Q-Learning

â”œâ”€â”€ agents/                     # Agentes del sistemaâ”‚   â”œâ”€â”€ entrenamiento.py          # Ciclos de entrenamiento

â”‚   â”œâ”€â”€ impala.py              # Comportamiento del impalaâ”‚   â””â”€â”€ recompensas.py            # Sistema de recompensas

â”‚   â””â”€â”€ leon.py                # Comportamiento del leÃ³n (aprendizaje)â”‚

â”‚â”œâ”€â”€ ui/                            # âœ… Interfaz de usuario

â”œâ”€â”€ knowledge/                  # Sistema de conocimientoâ”‚   â”œâ”€â”€ __init__.py

â”‚   â”œâ”€â”€ base_conocimientos.py # Tabla Q y almacenamientoâ”‚   â”œâ”€â”€ entrenamiento_ui.py       # UI para entrenamiento

â”‚   â””â”€â”€ generalizacion.py     # GeneralizaciÃ³n de patronesâ”‚   â”œâ”€â”€ paso_a_paso.py            # VisualizaciÃ³n detallada

â”‚â”‚   â””â”€â”€ explicador.py             # ExplicaciÃ³n de decisiones

â”œâ”€â”€ learning/                   # Aprendizaje por refuerzoâ”‚

â”‚   â”œâ”€â”€ q_learning.py         # Algoritmo Q-Learningâ”œâ”€â”€ storage/                       # âœ… Persistencia

â”‚   â”œâ”€â”€ entrenamiento.py      # Ciclos de entrenamientoâ”‚   â”œâ”€â”€ __init__.py

â”‚   â””â”€â”€ recompensas.py        # Sistema de recompensasâ”‚   â”œâ”€â”€ guardado.py               # Guardar conocimiento

â”‚â”‚   â””â”€â”€ carga.py                  # Cargar conocimiento

â”œâ”€â”€ simulation/                 # Motor de simulaciÃ³nâ”‚

â”‚   â”œâ”€â”€ caceria.py            # Orquestador de incursionesâ”œâ”€â”€ tests/                         # âœ… Pruebas unitarias

â”‚   â”œâ”€â”€ tiempo.py             # GestiÃ³n de turnosâ”‚   â””â”€â”€ test_basico.py            # Suite de tests

â”‚   â””â”€â”€ verificador.py        # Condiciones de Ã©xito/fracasoâ”‚

â”‚â”œâ”€â”€ datos/                         # Directorio para guardados

â”œâ”€â”€ storage/                    # Persistenciaâ”‚   â””â”€â”€ (archivos .json generados)

â”‚   â”œâ”€â”€ guardado.py           # Guardar modelosâ”‚   â””â”€â”€ conocimiento_guardado.json

â”‚   â””â”€â”€ carga.py              # Cargar modelosâ”‚

â”‚â”œâ”€â”€ environment.py                 # Entorno del abrevadero âœ…

â”œâ”€â”€ ui/                        # Interfaces de usuarioâ”œâ”€â”€ main.py                        # Punto de entrada (prÃ³ximo)

â”‚   â”œâ”€â”€ entrenamiento_ui.py   # UI de entrenamientoâ”œâ”€â”€ requirements.txt               # Dependencias (prÃ³ximo)

â”‚   â”œâ”€â”€ interfaz_terminal_grid.py  # VisualizaciÃ³n ASCIIâ””â”€â”€ README.md                      # Este archivo

â”‚   â”œâ”€â”€ paso_a_paso.py        # Modo paso a paso (texto)```

â”‚   â””â”€â”€ explicador.py         # Explicador de decisiones

â”‚## âœ… MÃ³dulos Implementados

â”œâ”€â”€ modelos/                   # Modelos entrenados (generados)

â”‚   â””â”€â”€ *.json                # Conocimientos guardados### 1. **environment.py** - Entorno del Abrevadero

â”‚- GestiÃ³n de 8 posiciones + centro

â”œâ”€â”€ environment.py             # ConfiguraciÃ³n del abrevadero- CÃ¡lculo de distancias y Ã¡ngulos

â”œâ”€â”€ main.py                    # Punto de entrada- VerificaciÃ³n de lÃ­nea de visiÃ³n

â””â”€â”€ requirements.txt           # Dependencias- Sistema de coordenadas cartesianas

```

### 2. **agents/impala.py** - Agente Impala

## ğŸ® GuÃ­a de Uso- Acciones: ver izq/der/frente, beber agua, huir

- Ãngulo de visiÃ³n limitado (120Â°)

### 1. Entrenar un Nuevo LeÃ³n- Sistema de huida con aceleraciÃ³n progresiva

- GeneraciÃ³n de secuencias aleatorias

```bash

python main.py### 3. **agents/leon.py** - Agente LeÃ³n

# Seleccionar: 1 (Sistema de Entrenamiento)- Acciones: avanzar, esconderse, atacar, situarse

# Elegir: 1 (Nuevo entrenamiento)- Velocidad de avance: 1 cuadro/T

# Configurar episodios: 100000 (recomendado)- Velocidad de ataque: 2 cuadros/T

# Seleccionar posiciÃ³n inicial: 0 (todas)- Control de visibilidad (escondido/visible)

# Esperar... (muestra progreso en tiempo real)

# Guardar modelo con nombre descriptivo### 4. **simulation/tiempo.py** - GestiÃ³n de Tiempo

```- Registro de eventos por unidad de tiempo T

- Historia completa de la simulaciÃ³n

**ParÃ¡metros de entrenamiento:**- GeneraciÃ³n de resÃºmenes

- **alpha (Î±)**: 0.05 - Tasa de aprendizaje

- **gamma (Î³)**: 0.9 - Factor de descuento### 5. **simulation/verificador.py** - Verificador de Condiciones

- **epsilon (Îµ)**: 0.01 - Tasa de exploraciÃ³n final- VerificaciÃ³n de condiciones de huida

- DetecciÃ³n de Ã©xito/fracaso de cacerÃ­a

**Resultados tÃ­picos:**- CÃ¡lculo de estado del mundo

- **10,000 episodios**: ~6-8% de Ã©xito

- **100,000 episodios**: ~10-12% de Ã©xito## ğŸ¯ Siguiente Fase de Desarrollo

- **500,000+ episodios**: ~12-15% de Ã©xito

### PrÃ³ximos mÃ³dulos a implementar:

### 2. Visualizar CacerÃ­a con LeÃ³n Entrenado

1. **simulation/caceria.py** - Orquestador principal

```bash   - Coordina acciones de impala y leÃ³n

python main.py   - Ejecuta una incursiÃ³n completa

# Seleccionar: 2 (SimulaciÃ³n Visual)   - Determina resultado final

# Elegir modelo entrenado de la lista

# Seleccionar posiciÃ³n inicial del leÃ³n2. **knowledge/** - Sistema de conocimiento

# Observar la cacerÃ­a en el grid 19Ã—19   - RepresentaciÃ³n de estados

# Presionar Enter para avanzar cada turno   - Almacenamiento de experiencias

```   - GeneralizaciÃ³n de patrones



**Leyenda de visualizaciÃ³n:**3. **learning/** - Aprendizaje por refuerzo

- ğŸ¦ LeÃ³n   - Algoritmo Q-Learning

- ğŸ¦Œ Impala   - Sistema de recompensas

- â–“ Abrevadero (centro)   - Ciclos de entrenamiento

- â–‘ Ãrea de visiÃ³n del impala

- â—‹ Trayectoria del leÃ³n4. **ui/** - Interfaces de usuario

   - Modo entrenamiento

### 3. Continuar Entrenamiento Existente   - Modo paso a paso

   - Explicador de decisiones

```bash

python main.py## ğŸ® Uso del Sistema

# Seleccionar: 1 (Sistema de Entrenamiento)

# Elegir: 2 (Continuar entrenamiento)### MenÃº Principal

# Seleccionar modelo a continuar

# Agregar mÃ¡s episodios```bash

# Guardar progresopython main.py

``````



## ğŸ§  Sistema de AprendizajeEl programa ofrece 5 opciones:



### Q-Learning1. **Sistema de Entrenamiento** - Entrenar al leÃ³n con miles de episodios

2. **VisualizaciÃ³n Paso a Paso** - Ver cacerÃ­as sin entrenamiento (decisiones aleatorias)

El leÃ³n aprende mediante la ecuaciÃ³n de Bellman:3. **VisualizaciÃ³n con LeÃ³n Entrenado** - Ver cacerÃ­as con leÃ³n que usa conocimiento aprendido

4. **Acerca del Proyecto** - InformaciÃ³n detallada

```5. **Salir**

Q(s,a) â† Q(s,a) + Î±[r + Î³Â·max Q(s',a') - Q(s,a)]

```### Ejemplo: Entrenamiento



**Donde:**```bash

- `s`: Estado actual (posiciÃ³n leÃ³n, distancia impala, acciÃ³n impala, visibilidad)python main.py

- `a`: AcciÃ³n tomada (avanzar, esconderse, atacar)# Seleccionar opciÃ³n 1

- `r`: Recompensa obtenida# Configurar nÃºmero de episodios (ej: 1000)

- `s'`: Estado siguiente# Seleccionar posiciones iniciales (Enter para todas)

- `Î±`: Tasa de aprendizaje (0.05)# Esperar a que termine el entrenamiento

- `Î³`: Factor de descuento (0.9)# Guardar el conocimiento aprendido

```

### Sistema de Recompensas

### Ejemplo: VisualizaciÃ³n con LeÃ³n Entrenado

| Evento | Recompensa |

|--------|-----------|```bash

| âœ… CacerÃ­a exitosa | +100 |python main.py

| âŒ Impala escapa | -50 |# Seleccionar opciÃ³n 3

| âš ï¸ Impala detecta al leÃ³n | -10 |# Elegir un entrenamiento guardado

| ğŸ“ ReducciÃ³n de distancia | +1 |# Seleccionar posiciÃ³n inicial

| ğŸƒ Impala huye | -5 |# Ver cÃ³mo el leÃ³n aplica lo aprendido

```

### GeneralizaciÃ³n

## ğŸ§ª Tests

El sistema abstrae estados especÃ­ficos en **patrones generales**:

- Distancias se redondean a 0.5 cuadros```bash

- Estados similares comparten conocimiento# Ejecutar suite de tests

- Tabla Q mÃ¡s compacta y eficientepython tests/test_basico.py



## ğŸŒ Entorno de SimulaciÃ³n# O probar mÃ³dulos individuales

python environment.py

### Abrevaderopython agents/leon.py

python knowledge/base_conocimientos.py

- **Grid**: 19Ã—19 cuadrospython learning/q_learning.py

- **Centro**: (9.5, 9.5) - PosiciÃ³n del abrevadero```

- **RADIO**: 9.5 cuadros - Distancia inicial leÃ³n-impala

- **Coordenadas**: Polares (r, Î¸) para el leÃ³n, Cartesianas (x, y) para visualizaciÃ³n## ğŸ“‹ Reglas del Sistema



### Posiciones Iniciales del LeÃ³n### Acciones por Turno (Tiempo T)

1. Impala actÃºa primero

El leÃ³n puede iniciar en 8 posiciones alrededor del abrevadero:2. LeÃ³n reacciona

- **PosiciÃ³n 1**: Norte (0Â°)3. Sistema verifica estado del mundo

- **PosiciÃ³n 2**: Noreste (45Â°)

- **PosiciÃ³n 3**: Este (90Â°)### Condiciones de Huida del Impala

- **PosiciÃ³n 4**: Sureste (135Â°)- Ve al leÃ³n (dentro de Ã¡ngulo de visiÃ³n Y NO escondido)

- **PosiciÃ³n 5**: Sur (180Â°)- LeÃ³n inicia ataque

- **PosiciÃ³n 6**: Suroeste (225Â°)- Distancia < 3 cuadros

- **PosiciÃ³n 7**: Oeste (270Â°)

- **PosiciÃ³n 8**: Noroeste (315Â°)### Fin de IncursiÃ³n

- **Ã‰xito**: LeÃ³n alcanza al impala

### Comportamiento del Impala- **Fracaso**: LeÃ³n no puede alcanzar al impala



**Secuencia programada:**## ğŸ“ Aprendizaje

1. Ver izquierda

2. Ver derechaEl leÃ³n debe aprender:

3. Ver al frente- âœ… CuÃ¡ndo avanzar vs esconderse

4. Beber agua (vulnerable - no ve)- âœ… Desde quÃ© distancia atacar

5. Ver izquierda- âœ… CÃ³mo aprovechar el comportamiento del impala

6. Ver al frente- âŒ NO se programa explÃ­citamente la estrategia

7. Beber agua

## ğŸ“Š Modos de OperaciÃ³n

**Ãngulo de visiÃ³n:** 120Â° (puede ver en un cono frontal)

### 1. Fase de Entrenamiento

**Condiciones de huida:**- Ciclos automÃ¡ticos (100, 1000, 10000+ incursiones)

- Detecta al leÃ³n visible dentro de su Ã¡ngulo de visiÃ³n- Posiciones iniciales configurables

- LeÃ³n inicia ataque- Comportamiento impala: aleatorio o programado

- Distancia < 3 cuadros (DISTANCIA_MINIMA_HUIDA)

### 2. CacerÃ­a Paso a Paso

**Velocidad de huida:** 1.5 cuadros/turno (mÃ¡s rÃ¡pido que el leÃ³n)- VisualizaciÃ³n T1, T2, T3... Tn

- ExplicaciÃ³n de decisiones del leÃ³n

### Acciones del LeÃ³n- Exportar base de conocimientos



| AcciÃ³n | Velocidad | DescripciÃ³n |## ğŸ› ï¸ TecnologÃ­as

|--------|-----------|-------------|

| **Avanzar** | 1 cuadro/T | Se acerca al abrevadero (visible) |- **Python 3.8+**

| **Esconderse** | 0 cuadros/T | Se oculta en su posiciÃ³n (invisible) |- Type hints

| **Atacar** | 2 cuadros/T | Sprint final hacia el impala |- Docstrings formato Google

- PEP 8

## ğŸ“Š Archivos Guardados

## ğŸ“ Estado del Proyecto

Cada modelo entrenado genera 3 archivos en `modelos/`:

- [x] Arquitectura modular definida

1. **`nombre_conocimiento.json`**: Tabla Q completa- [x] Instrucciones para Copilot

2. **`nombre_config.json`**: ConfiguraciÃ³n (RADIO, parÃ¡metros de aprendizaje)- [x] MÃ³dulo de entorno

3. **`nombre_reporte.txt`**: MÃ©tricas y estadÃ­sticas del entrenamiento- [x] Agentes bÃ¡sicos (leÃ³n e impala)

- [x] Sistema de tiempo

### Formato de ConfiguraciÃ³n- [x] Verificador de condiciones

- [ ] MÃ³dulo de cacerÃ­a completo

```json- [ ] Sistema de conocimiento

{- [ ] Aprendizaje Q-Learning

  "radio": 9.5,- [ ] Interfaz de usuario

  "angulo_vision": 120,- [ ] Persistencia

  "distancia_minima_huida": 3,- [ ] Pruebas unitarias

  "cacerias_exitosas": 10450,- [ ] DocumentaciÃ³n completa

  "total_cacerias": 100000,

  "experiencias_unicas": 145135---

}

```**PrÃ³ximo paso**: Implementar `simulation/caceria.py` para orquestar una incursiÃ³n completa.


### VerificaciÃ³n de Compatibilidad

El sistema verifica que el **RADIO** del modelo coincida con el entorno actual:
- âœ… RADIO = 9.5 â†’ Compatible
- âŒ RADIO = 5.0 â†’ Incompatible (muestra advertencia)

## ğŸ”¬ Detalles TÃ©cnicos

### Estados del Mundo

Un estado se define por:
- **posicion_leon**: (r, Î¸) - Coordenadas polares
- **distancia_impala**: Distancia leÃ³n-impala (redondeada)
- **accion_impala**: AcciÃ³n actual del impala
- **leon_escondido**: Boolean (visible/invisible)
- **impala_puede_ver**: Boolean (dentro/fuera de visiÃ³n)

### Tabla Q

Estructura: `Dict[Estado, Dict[Accion, float]]`
- **Claves**: Estados del mundo (dataclass hashable)
- **Valores**: Diccionario de valores Q por acciÃ³n
- **TamaÃ±o tÃ­pico**: 145,000-675,000 experiencias Ãºnicas

### PolÃ­tica de ExploraciÃ³n

**Epsilon-greedy decreciente:**
- Inicio: Îµ = 1.0 (100% exploraciÃ³n)
- Decremento: Îµ -= 0.9/num_episodios por episodio
- Final: Îµ = 0.1 (10% exploraciÃ³n, 90% explotaciÃ³n)

## ğŸ† Resultados de Ejemplo

### Entrenamiento EM4 (100,000 episodios)

```
Total de cacerÃ­as: 100,000
CacerÃ­as exitosas: 10,450
Tasa de Ã©xito: 10.45%
Experiencias Ãºnicas: 145,135
Tiempo de entrenamiento: ~15 minutos
```

### ProgresiÃ³n TÃ­pica

| Episodios | Tasa de Ã‰xito |
|-----------|---------------|
| 1,000 | 3-4% |
| 10,000 | 6-8% |
| 50,000 | 9-10% |
| 100,000 | 10-12% |
| 500,000 | 12-15% |

## ğŸ› SoluciÃ³n de Problemas

### Error: "El RADIO del modelo no coincide"

El modelo fue entrenado con RADIO=5.0 (obsoleto). Entrenar nuevo modelo con RADIO=9.5.

### El impala siempre escapa

Normal con pocos episodios de entrenamiento. Entrenar con 100,000+ episodios.

### VisualizaciÃ³n no muestra colores

Terminal no soporta ANSI. Los emojis y ASCII seguirÃ¡n funcionando.

### Error de importaciÃ³n

Verificar que estÃ¡s ejecutando desde el directorio raÃ­z del proyecto.

## ğŸ“š Referencias

- **Q-Learning**: Watkins, C.J.C.H. (1989). Learning from Delayed Rewards
- **Aprendizaje por Refuerzo**: Sutton & Barto (2018). Reinforcement Learning: An Introduction

## ğŸ‘¨â€ğŸ’» Desarrollo

### PrÃ³ximas Mejoras

- [ ] Interfaz grÃ¡fica con pygame/tkinter
- [ ] MÃºltiples impalas en el abrevadero
- [ ] Terreno con obstÃ¡culos
- [ ] Deep Q-Networks (DQN)
- [ ] Algoritmos alternativos (SARSA, Actor-Critic)

### Contribuir

El proyecto fue desarrollado como trabajo final de Sistemas Inteligentes. Las contribuciones son bienvenidas.

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto para fines educativos.

---

**Desarrollado con:** Python 3.13 | Q-Learning | Aprendizaje por Refuerzo

**Estado:** âœ… Funcional - 10.45% tasa de Ã©xito con 100K episodios
