# ğŸ¦ LeÃ³n vs Impala - Sistema de Aprendizaje por Refuerzo# LeÃ³n vs Impala - Sistema de Aprendizaje por Refuerzo



Sistema de aprendizaje automÃ¡tico donde un leÃ³n joven aprende a cazar un impala en un abrevadero mediante **Q-Learning**. El leÃ³n no tiene estrategia programada, sino que aprende por experiencia tras miles de cacerÃ­as simuladas.Sistema de aprendizaje automÃ¡tico donde un leÃ³n joven aprende a cazar un impala en un abrevadero mediante aprendizaje por refuerzo (Q-Learning).



## ğŸ¯ CaracterÃ­sticas Principales## ğŸš€ Inicio RÃ¡pido



- âœ… **Aprendizaje por Refuerzo**: Q-Learning con exploraciÃ³n epsilon-greedy```bash

- âœ… **VisualizaciÃ³n en Terminal**: Grid ASCII 19Ã—19 con colores ANSI# Ejecutar el programa principal

- âœ… **Persistencia de Modelos**: Guarda y carga conocimientos aprendidospython main.py

- âœ… **Sistema de GeneralizaciÃ³n**: Abstrae experiencias en patrones reutilables

- âœ… **MÃ©tricas de Rendimiento**: Seguimiento de tasa de Ã©xito durante entrenamiento# Ejecutar tests

- âœ… **SimulaciÃ³n Realista**: Coordenadas polares, lÃ­nea de visiÃ³n, fÃ­sica de movimientopython tests/test_basico.py

```

## ğŸš€ Inicio RÃ¡pido

## ğŸ“‹ Requisitos

### Requisitos

- Python 3.8 o superior- Python 3.8 o superior

- Sin dependencias externas (solo librerÃ­as estÃ¡ndar)- Solo librerÃ­as estÃ¡ndar (sin dependencias externas)



### InstalaciÃ³n## ğŸ“ Estructura del Proyecto



```bash```

# Clonar el repositorioLeonvsImapala/

cd /home/holly/Proyectos/LeonvsImapalaâ”œâ”€â”€ .github/

â”‚   â””â”€â”€ copilot-instructions.md    # Instrucciones para GitHub Copilot

# El proyecto estÃ¡ listo para ejecutarseâ”‚

python main.pyâ”œâ”€â”€ agents/                         # Agentes del sistema

```â”‚   â”œâ”€â”€ __init__.py

â”‚   â”œâ”€â”€ impala.py                  # Comportamiento del impala

### Uso BÃ¡sicoâ”‚   â””â”€â”€ leon.py                    # Comportamiento del leÃ³n

â”‚

```bashâ”œâ”€â”€ simulation/                     # Motor de simulaciÃ³n

python main.pyâ”‚   â”œâ”€â”€ __init__.py

```â”‚   â”œâ”€â”€ caceria.py                 # LÃ³gica de incursiÃ³n de cacerÃ­a

â”‚   â”œâ”€â”€ tiempo.py                  # GestiÃ³n de unidades de tiempo T

**MenÃº Principal:**â”‚   â””â”€â”€ verificador.py             # VerificaciÃ³n de condiciones

1. **Sistema de Entrenamiento** - Entrenar al leÃ³n (recomendado: 10,000-100,000 episodios)â”‚

2. **SimulaciÃ³n Visual** - Ver cacerÃ­as con visualizaciÃ³n ASCII en tiempo realâ”œâ”€â”€ knowledge/                      # âœ… Sistema de conocimiento

3. **Acerca del Proyecto** - InformaciÃ³n tÃ©cnica detalladaâ”‚   â”œâ”€â”€ __init__.py

4. **Salir**â”‚   â”œâ”€â”€ base_conocimientos.py     # Almacenamiento de experiencias

â”‚   â””â”€â”€ generalizacion.py         # AbstracciÃ³n de patrones

## ğŸ“ Estructura del Proyectoâ”‚

â”œâ”€â”€ learning/                       # âœ… Aprendizaje por refuerzo

```â”‚   â”œâ”€â”€ __init__.py

LeonvsImapala/â”‚   â”œâ”€â”€ q_learning.py             # Algoritmo Q-Learning

â”œâ”€â”€ agents/                     # Agentes del sistemaâ”‚   â”œâ”€â”€ entrenamiento.py          # Ciclos de entrenamiento

â”‚   â”œâ”€â”€ impala.py              # Comportamiento del impalaâ”‚   â””â”€â”€ recompensas.py            # Sistema de recompensas

â”‚   â””â”€â”€ leon.py                # Comportamiento del leÃ³n (aprendizaje)â”‚

â”‚â”œâ”€â”€ ui/                            # âœ… Interfaz de usuario

â”œâ”€â”€ knowledge/                  # Sistema de conocimientoâ”‚   â”œâ”€â”€ __init__.py

â”‚   â”œâ”€â”€ base_conocimientos.py # Tabla Q y almacenamientoâ”‚   â”œâ”€â”€ entrenamiento_ui.py       # UI para entrenamiento

â”‚   â””â”€â”€ generalizacion.py     # GeneralizaciÃ³n de patronesâ”‚   â”œâ”€â”€ paso_a_paso.py            # VisualizaciÃ³n detallada

â”‚â”‚   â””â”€â”€ explicador.py             # ExplicaciÃ³n de decisiones

â”œâ”€â”€ learning/                   # Aprendizaje por refuerzoâ”‚

â”‚   â”œâ”€â”€ q_learning.py         # Algoritmo Q-Learningâ”œâ”€â”€ storage/                       # âœ… Persistencia

â”‚   â”œâ”€â”€ entrenamiento.py      # Ciclos de entrenamientoâ”‚   â”œâ”€â”€ __init__.py

â”‚   â””â”€â”€ recompensas.py        # Sistema de recompensasâ”‚   â”œâ”€â”€ guardado.py               # Guardar conocimiento

â”‚â”‚   â””â”€â”€ carga.py                  # Cargar conocimiento

â”œâ”€â”€ simulation/                 # Motor de simulaciÃ³nâ”‚

â”‚   â”œâ”€â”€ caceria.py            # Orquestador de incursionesâ”œâ”€â”€ tests/                         # âœ… Pruebas unitarias

â”‚   â”œâ”€â”€ tiempo.py             # GestiÃ³n de turnosâ”‚   â””â”€â”€ test_basico.py            # Suite de tests

â”‚   â””â”€â”€ verificador.py        # Condiciones de Ã©xito/fracasoâ”‚

â”‚â”œâ”€â”€ datos/                         # Directorio para guardados

â”œâ”€â”€ storage/                    # Persistenciaâ”‚   â””â”€â”€ (archivos .json generados)

â”‚   â”œâ”€â”€ guardado.py           # Guardar modelosâ”‚   â””â”€â”€ conocimiento_guardado.json

â”‚   â””â”€â”€ carga.py              # Cargar modelosâ”‚

â”‚â”œâ”€â”€ environment.py                 # Entorno del abrevadero âœ…

â”œâ”€â”€ ui/                        # Interfaces de usuarioâ”œâ”€â”€ main.py                        # Punto de entrada (prÃ³ximo)

â”‚   â”œâ”€â”€ entrenamiento_ui.py   # UI de entrenamientoâ”œâ”€â”€ requirements.txt               # Dependencias (prÃ³ximo)

â”‚   â”œâ”€â”€ interfaz_terminal_grid.py  # VisualizaciÃ³n ASCIIâ””â”€â”€ README.md                      # Este archivo

â”‚   â”œâ”€â”€ paso_a_paso.py        # Modo paso a paso (texto)```

â”‚   â””â”€â”€ explicador.py         # Explicador de decisiones

â”‚## âœ… MÃ³dulos Implementados

â”œâ”€â”€ modelos/                   # Modelos entrenados (generados)

â”‚   â””â”€â”€ *.json                # Conocimientos guardados### 1. **environment.py** - Entorno del Abrevadero

â”‚- GestiÃ³n de 8 posiciones + centro

â”œâ”€â”€ environment.py             # ConfiguraciÃ³n del abrevadero- CÃ¡lculo de distancias y Ã¡ngulos

â”œâ”€â”€ main.py                    # Punto de entrada- VerificaciÃ³n de lÃ­nea de visiÃ³n

â””â”€â”€ requirements.txt           # Dependencias- Sistema de coordenadas cartesianas

```

### 2. **agents/impala.py** - Agente Impala

## ğŸ® GuÃ­a de Uso- Acciones: ver izq/der/frente, beber agua, huir

- Ãngulo de visiÃ³n limitado (120Â°)

### 1. Entrenar un Nuevo LeÃ³n- Sistema de huida con aceleraciÃ³n progresiva

- GeneraciÃ³n de secuencias aleatorias

```bash

python main.py### 3. **agents/leon.py** - Agente LeÃ³n

# Seleccionar: 1 (Sistema de Entrenamiento)- Acciones: avanzar, esconderse, atacar, situarse

# Elegir: 1 (Nuevo entrenamiento)- Velocidad de avance: 1 cuadro/T

# Configurar episodios: 100000 (recomendado)- Velocidad de ataque: 2 cuadros/T

# Seleccionar posiciÃ³n inicial: 0 (todas)- Control de visibilidad (escondido/visible)

# Esperar... (muestra progreso en tiempo real)

# Guardar modelo con nombre descriptivo### 4. **simulation/tiempo.py** - GestiÃ³n de Tiempo

```- Registro de eventos por unidad de tiempo T

- Historia completa de la simulaciÃ³n

**ParÃ¡metros de entrenamiento:**- GeneraciÃ³n de resÃºmenes

- **alpha (Î±)**: 0.05 - Tasa de aprendizaje

- **gamma (Î³)**: 0.9 - Factor de descuento### 5. **simulation/verificador.py** - Verificador de Condiciones

- **epsilon (Îµ)**: 0.01 - Tasa de exploraciÃ³n final- VerificaciÃ³n de condiciones de huida

- DetecciÃ³n de Ã©xito/fracaso de cacerÃ­a

**Resultados tÃ­picos:**- CÃ¡lculo de estado del mundo

- **10,000 episodios**: ~6-8% de Ã©xito

- **100,000 episodios**: ~10-12% de Ã©xito## ğŸ¯ Siguiente Fase de Desarrollo

- **500,000+ episodios**: ~12-15% de Ã©xito

### PrÃ³ximos mÃ³dulos a implementar:

### 2. Visualizar CacerÃ­a con LeÃ³n Entrenado

1. **simulation/caceria.py** - Orquestador principal

```bash   - Coordina acciones de impala y leÃ³n

python main.py   - Ejecuta una incursiÃ³n completa

# Seleccionar: 2 (SimulaciÃ³n Visual)   - Determina resultado final

# Elegir modelo entrenado de la lista

# Seleccionar posiciÃ³n inicial del leÃ³n2. **knowledge/** - Sistema de conocimiento

# Observar la cacerÃ­a en el grid 19Ã—19   - RepresentaciÃ³n de estados

# Presionar Enter para avanzar cada turno   - Almacenamiento de experiencias

```   - GeneralizaciÃ³n de patrones



**Leyenda de visualizaciÃ³n:**3. **learning/** - Aprendizaje por refuerzo

- ğŸ¦ LeÃ³n   - Algoritmo Q-Learning

- ğŸ¦Œ Impala   - Sistema de recompensas

- â–“ Abrevadero (centro)   - Ciclos de entrenamiento

- â–‘ Ãrea de visiÃ³n del impala

- â—‹ Trayectoria del leÃ³n4. **ui/** - Interfaces de usuario

   - Modo entrenamiento

### 3. Continuar Entrenamiento Existente   - Modo paso a paso

   - Explicador de decisiones

```bash

python main.py## ğŸ® Uso del Sistema

# Seleccionar: 1 (Sistema de Entrenamiento)

# Elegir: 2 (Continuar entrenamiento)### MenÃº Principal

# Seleccionar modelo a continuar

# Agregar mÃ¡s episodios```bash

# Guardar progresopython main.py

``````



## ğŸ§  Sistema de Aprendizaje

### Â¿QuÃ© es Q-Learning?

**Q-Learning** es un algoritmo de **Aprendizaje por Refuerzo** (Reinforcement Learning) que permite a un agente aprender la mejor acciÃ³n a tomar en cada situaciÃ³n mediante prueba y error, sin necesidad de un modelo explÃ­cito del entorno.

#### Concepto Fundamental

El leÃ³n aprende construyendo una **tabla Q** que mapea cada combinaciÃ³n de estado-acciÃ³n a un valor que representa "quÃ© tan buena" es esa acciÃ³n en ese estado. A travÃ©s de miles de cacerÃ­as, el leÃ³n descubre quÃ© acciones maximizan su probabilidad de Ã©xito.

#### Proceso de Aprendizaje

1. **ExploraciÃ³n**: Al inicio, el leÃ³n prueba acciones aleatorias para descubrir el entorno
2. **Experiencia**: Cada cacerÃ­a genera experiencias (estado â†’ acciÃ³n â†’ recompensa â†’ nuevo estado)
3. **ActualizaciÃ³n**: Los valores Q se actualizan basÃ¡ndose en las recompensas obtenidas
4. **ExplotaciÃ³n**: Con el tiempo, el leÃ³n prefiere acciones que histÃ³ricamente funcionaron mejor
5. **Convergencia**: DespuÃ©s de muchos episodios, el leÃ³n desarrolla una estrategia Ã³ptima

### La EcuaciÃ³n de Bellman

El leÃ³n aprende mediante la **ecuaciÃ³n de Bellman** para actualizaciÃ³n de valores Q:

```
Q(s,a) â† Q(s,a) + Î±[r + Î³Â·max Q(s',a') - Q(s,a)]
```

**Componentes de la ecuaciÃ³n:**

- **`Q(s,a)`**: Valor Q actual para el estado `s` y acciÃ³n `a`
- **`Î±`** (alpha): **Tasa de aprendizaje** = 0.05
  - Controla quÃ© tan rÃ¡pido se actualizan los valores
  - Valor bajo (0.05) = aprendizaje gradual y estable
  
- **`Î³`** (gamma): **Factor de descuento** = 0.9
  - Importancia de recompensas futuras vs inmediatas
  - 0.9 = el leÃ³n valora mucho las consecuencias futuras
  
- **`r`**: **Recompensa inmediata** obtenida
  - +100 por captura exitosa
  - -50 por fracaso
  - +1 por acercarse
  
- **`s'`**: **Nuevo estado** despuÃ©s de la acciÃ³n
- **`max Q(s',a')`**: Mejor valor Q posible en el nuevo estado
  - EstimaciÃ³n del valor futuro Ã³ptimo

#### InterpretaciÃ³n Intuitiva

La ecuaciÃ³n dice: *"El valor de tomar la acciÃ³n A en el estado S es la recompensa inmediata mÃ¡s el mejor valor que puedo obtener en el futuro, ajustado por lo que ya sabÃ­a"*.

### PolÃ­tica Epsilon-Greedy

El leÃ³n balancea **exploraciÃ³n** vs **explotaciÃ³n** mediante epsilon (Îµ):

```python
if random() < epsilon:
    acciÃ³n = aleatoria()  # EXPLORAR: probar algo nuevo
else:
    acciÃ³n = mejor_conocida()  # EXPLOTAR: usar lo aprendido
```

**Decaimiento de Epsilon:**
- Inicio: Îµ = 1.0 (100% exploraciÃ³n)
- Decremento: Îµ -= 0.9/episodios_totales
- Final: Îµ = 0.1 (10% exploraciÃ³n, 90% explotaciÃ³n)

Esto significa que el leÃ³n empieza probando todo aleatoriamente, y gradualmente confÃ­a mÃ¡s en su experiencia.

### RepresentaciÃ³n de Estados

Cada estado captura la situaciÃ³n completa del mundo:

```python
Estado = {
    'posicion_leon': int,           # 1-8 (posiciÃ³n discreta)
    'distancia_impala': float,      # Redondeada a 0.5 unidades
    'accion_impala': str,           # 'ver_izq', 'ver_der', 'beber', etc.
    'leon_escondido': bool,         # Â¿LeÃ³n oculto?
    'impala_puede_ver': bool        # Â¿Impala puede ver al leÃ³n?
}
```

La tabla Q almacena valores para cada combinaciÃ³n posible de (Estado, AcciÃ³n).

### Ejemplo de Aprendizaje

**Episodio 1** (sin experiencia):
```
Estado: LeÃ³n en pos 1, distancia 9.5, impala bebiendo
Q(estado, atacar) = 0 (valor inicial)
AcciÃ³n: Atacar (aleatorio)
Resultado: Impala detecta y escapa (-50)
ActualizaciÃ³n: Q(estado, atacar) = -2.5 (ahora sabe que atacar lejos es malo)
```

**Episodio 1000** (con experiencia):
```
Estado: LeÃ³n en pos 1, distancia 9.5, impala bebiendo
Q(estado, esconderse) = 45 (mejor opciÃ³n conocida)
Q(estado, avanzar) = 30
Q(estado, atacar) = -2.5 (ya aprendiÃ³ que es mala idea)
AcciÃ³n: Esconderse (explota conocimiento)
```

### Sistema de Recompensas

### Ejemplo: VisualizaciÃ³n con LeÃ³n Entrenado

| Evento | Recompensa |

|--------|-----------|```bash

| âœ… CacerÃ­a exitosa | +100 |python main.py

| âŒ Impala escapa | -50 |# Seleccionar opciÃ³n 3

| âš ï¸ Impala detecta al leÃ³n | -10 |# Elegir un entrenamiento guardado

| ğŸ“ ReducciÃ³n de distancia | +1 |# Seleccionar posiciÃ³n inicial

| ğŸƒ Impala huye | -5 |# Ver cÃ³mo el leÃ³n aplica lo aprendido

```

### GeneralizaciÃ³n

## ğŸ§ª Tests Unitarios

El proyecto incluye una suite completa de tests que valida todas las funciones crÃ­ticas del sistema.

### Ejecutar Tests

```bash
# Ejecutar suite completa de tests
python tests/test_basico.py
```

### Tests Incluidos

#### 1. **Test de Abrevadero** âœ…
- ValidaciÃ³n de coordenadas de las 8 posiciones
- CÃ¡lculo correcto de distancias
- VerificaciÃ³n del RADIO = 9.5 unidades

#### 2. **Test de Acciones del LeÃ³n** âœ…
- Avanzar: Movimiento de 1 cuadro/turno
- Esconderse: Cambio de estado de visibilidad
- Atacar: Velocidad de 2 cuadros/turno

#### 3. **Test de Acciones del Impala** âœ…
- Ver (izquierda, derecha, frente)
- Beber agua
- Huir con aceleraciÃ³n progresiva

#### 4. **Test de Base de Conocimientos** âœ…
- Almacenamiento de estados y valores Q
- RecuperaciÃ³n de mejores acciones
- ActualizaciÃ³n de tabla Q

#### 5. **Test de Q-Learning** âœ…
- SelecciÃ³n de acciones (exploraciÃ³n vs explotaciÃ³n)
- PolÃ­tica epsilon-greedy
- ValidaciÃ³n de tipos de decisiÃ³n

#### 6. **Test de Sistema de Recompensas** âœ…
- Recompensa por Ã©xito: +100
- PenalizaciÃ³n por fracaso: -50
- Recompensas por acercamiento

#### 7. **Test de CacerÃ­a Completa** âœ…
- EjecuciÃ³n completa de una cacerÃ­a
- ValidaciÃ³n de resultados (Ã©xito/fracaso)
- Estrategia simple de prueba

#### 8. **Test de CacerÃ­a Turno a Turno** âœ…
- EjecuciÃ³n de turnos individuales
- Registro de eventos en el tiempo
- VerificaciÃ³n de historial

### Resultados Esperados

```
Ejecutando tests bÃ¡sicos...

âœ“ Abrevadero - Coordenadas
âœ“ Abrevadero - Distancia
âœ“ LeÃ³n - Acciones
âœ“ Impala - Acciones
âœ“ Base Conocimientos
âœ“ Q-Learning - SelecciÃ³n
âœ“ Sistema Recompensas
âœ“ CacerÃ­a Completa
âœ“ CacerÃ­a Turno a Turno

==================================================
Resultados: 9 exitosos, 0 fallidos
==================================================
```

### Cobertura

Los tests cubren:
- âœ… **Entorno**: Coordenadas, distancias, geometrÃ­a
- âœ… **Agentes**: Todas las acciones de leÃ³n e impala
- âœ… **Aprendizaje**: Q-Learning, recompensas, estados
- âœ… **SimulaciÃ³n**: CacerÃ­as completas y por turnos
- âœ… **Conocimiento**: Almacenamiento y recuperaciÃ³n

### GeneralizaciÃ³n

El sistema abstrae estados especÃ­ficos en **patrones generales**:

- Distancias se redondean a 0.5 cuadros
- Estados similares comparten conocimiento
- Tabla Q mÃ¡s compacta y eficiente

## ğŸŒ Entorno de SimulaciÃ³n

### Sistema de Coordenadas Polares

El proyecto utiliza **coordenadas polares** para representar las posiciones del leÃ³n alrededor del abrevadero, lo cual es mÃ¡s natural para este escenario circular.

#### Â¿Por quÃ© Coordenadas Polares?

En lugar de usar coordenadas cartesianas tradicionales (x, y), usamos **coordenadas polares (r, Î¸)**:

- **`r`** (radio): Distancia desde el centro del abrevadero
- **`Î¸`** (theta): Ãngulo en grados (0Â° = Norte)

**Ventajas para este problema:**

1. **Naturalidad del escenario**: El abrevadero es circular, el leÃ³n rodea al impala
2. **SimplificaciÃ³n de cÃ¡lculos**: Las 8 posiciones iniciales estÃ¡n a la misma distancia (r = 9.5)
3. **Movimiento intuitivo**: Avanzar = reducir r (acercarse al centro)
4. **RepresentaciÃ³n compacta**: Solo necesitamos Ã¡ngulo y distancia

#### Las 8 Posiciones Iniciales

El leÃ³n puede empezar en 8 posiciones equidistantes alrededor del abrevadero:

```
                    PosiciÃ³n 1
                      Î¸ = 0Â°
                      Norte
                        ğŸ¦
                        |
                        |
    Pos 8              |              Pos 2
    Î¸=315Â°             |              Î¸=45Â°
    Noroeste -------(CENTRO)------- Noreste
                    IMPALAğŸ¦Œ
                        |
    Pos 7              |              Pos 3
    Î¸=270Â°             |              Î¸=90Â°
    Oeste ----------(CENTRO)--------- Este
                        |
                        |
                    PosiciÃ³n 5
                      Î¸=180Â°
                       Sur
                    Pos 4  Pos 6
                   Î¸=135Â° Î¸=225Â°
```

**FÃ³rmula de conversiÃ³n:**
```python
Î¸ = (posicion - 1) Ã— 45Â°

PosiciÃ³n 1: Î¸ = 0Â°    (Norte)
PosiciÃ³n 2: Î¸ = 45Â°   (Noreste)
PosiciÃ³n 3: Î¸ = 90Â°   (Este)
...
PosiciÃ³n 8: Î¸ = 315Â°  (Noroeste)
```

#### ConversiÃ³n Polar â†’ Cartesiana

Para la visualizaciÃ³n en el grid 19Ã—19, convertimos coordenadas polares a cartesianas:

```python
x = r Ã— sin(Î¸)
y = r Ã— cos(Î¸)

# Ejemplo PosiciÃ³n 1 (Norte):
r = 9.5, Î¸ = 0Â°
x = 9.5 Ã— sin(0Â°) = 0
y = 9.5 Ã— cos(0Â°) = 9.5
Coordenadas: (0, 9.5)

# Ejemplo PosiciÃ³n 3 (Este):
r = 9.5, Î¸ = 90Â°
x = 9.5 Ã— sin(90Â°) = 9.5
y = 9.5 Ã— cos(90Â°) = 0
Coordenadas: (9.5, 0)
```

#### Movimiento del LeÃ³n

Cuando el leÃ³n **avanza** o **ataca**, se mueve en lÃ­nea recta hacia el centro:

```python
# Avanzar 1 cuadro:
nueva_r = r - 1
nueva_Î¸ = Î¸  # El Ã¡ngulo se mantiene

# Ejemplo: LeÃ³n en pos 1, avanza 3 turnos
Turno 0: r=9.5, Î¸=0Â° â†’ (0, 9.5)
Turno 1: r=8.5, Î¸=0Â° â†’ (0, 8.5)  # AvanzÃ³ 1
Turno 2: r=7.5, Î¸=0Â° â†’ (0, 7.5)  # AvanzÃ³ 1
Turno 3: r=6.5, Î¸=0Â° â†’ (0, 6.5)  # AvanzÃ³ 1
```

#### VisualizaciÃ³n en Grid 19Ã—19

El grid usa coordenadas cartesianas para facilitar la visualizaciÃ³n:

- **Centro del grid**: (9.5, 9.5)
- **Escala**: 1.9 (factor de conversiÃ³n polar â†’ grid)
- **Origen polar** (0, 0) â†’ **Centro grid** (9.5, 9.5)

```
Grid Cartesiano 19Ã—19:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Â· Â· Â· Â· Â· ğŸ¦ Â· Â· Â· Â· â”‚  â† LeÃ³n en (9.5, 18.05)
â”‚ Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· â”‚     Polar: r=9.5, Î¸=0Â°
â”‚ Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· â”‚
â”‚ Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· â”‚
â”‚ Â· Â· Â· Â· â–“â–“â–“ Â· Â· Â· Â· â”‚  â† Abrevadero
â”‚ Â· Â· Â· Â· â–“ğŸ¦Œâ–“ Â· Â· Â· Â· â”‚     Centro (9.5, 9.5)
â”‚ Â· Â· Â· Â· â–“â–“â–“ Â· Â· Â· Â· â”‚
â”‚ Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Abrevadero

- **Grid**: 19Ã—19 cuadros
- **Centro**: (9.5, 9.5) - PosiciÃ³n del abrevadero
- **RADIO**: 9.5 cuadros - Distancia inicial leÃ³n-impala
- **Coordenadas**: Polares (r, Î¸) para el leÃ³n, Cartesianas (x, y) para visualizaciÃ³n

### CÃ¡lculo de Distancias

La distancia leÃ³n-impala se calcula con la **fÃ³rmula euclidiana**:

```python
# Si leÃ³n estÃ¡ en (x_leon, y_leon) e impala en centro (0, 0)
distancia = âˆš(x_leonÂ² + y_leonÂ²)

# En coordenadas polares es simplemente:
distancia = r  (el radio actual del leÃ³n)
```

**Umbral de captura:** distancia â‰¤ 0.5 unidades

## ğŸ“‹ Reglas del Sistema



### Posiciones Iniciales del LeÃ³n### Acciones por Turno (Tiempo T)

1. Impala actÃºa primero

El leÃ³n puede iniciar en 8 posiciones alrededor del abrevadero:2. LeÃ³n reacciona

- **PosiciÃ³n 1**: Norte (0Â°)3. Sistema verifica estado del mundo

- **PosiciÃ³n 2**: Noreste (45Â°)

- **PosiciÃ³n 3**: Este (90Â°)### Condiciones de Huida del Impala

- **PosiciÃ³n 4**: Sureste (135Â°)- Ve al leÃ³n (dentro de Ã¡ngulo de visiÃ³n Y NO escondido)

- **PosiciÃ³n 5**: Sur (180Â°)- LeÃ³n inicia ataque

- **PosiciÃ³n 6**: Suroeste (225Â°)- Distancia < 3 cuadros

- **PosiciÃ³n 7**: Oeste (270Â°)

- **PosiciÃ³n 8**: Noroeste (315Â°)### Fin de IncursiÃ³n

- **Ã‰xito**: LeÃ³n alcanza al impala

### Comportamiento del Impala- **Fracaso**: LeÃ³n no puede alcanzar al impala



**Secuencia programada:**## ğŸ“ Aprendizaje

1. Ver izquierda

2. Ver derechaEl leÃ³n debe aprender:

3. Ver al frente- âœ… CuÃ¡ndo avanzar vs esconderse

4. Beber agua (vulnerable - no ve)- âœ… Desde quÃ© distancia atacar

5. Ver izquierda- âœ… CÃ³mo aprovechar el comportamiento del impala

6. Ver al frente- âŒ NO se programa explÃ­citamente la estrategia

7. Beber agua

## ğŸ“Š Modos de OperaciÃ³n

**Ãngulo de visiÃ³n:** 120Â° (puede ver en un cono frontal)

### 1. Fase de Entrenamiento

**Condiciones de huida:**- Ciclos automÃ¡ticos (100, 1000, 10000+ incursiones)

- Detecta al leÃ³n visible dentro de su Ã¡ngulo de visiÃ³n- Posiciones iniciales configurables

- LeÃ³n inicia ataque- Comportamiento impala: aleatorio o programado

- Distancia < 3 cuadros (DISTANCIA_MINIMA_HUIDA)

### 2. CacerÃ­a Paso a Paso

**Velocidad de huida:** 1.5 cuadros/turno (mÃ¡s rÃ¡pido que el leÃ³n)- VisualizaciÃ³n T1, T2, T3... Tn

- ExplicaciÃ³n de decisiones del leÃ³n

### Acciones del LeÃ³n- Exportar base de conocimientos



| AcciÃ³n | Velocidad | DescripciÃ³n |## ğŸ› ï¸ TecnologÃ­as

|--------|-----------|-------------|

| **Avanzar** | 1 cuadro/T | Se acerca al abrevadero (visible) |- **Python 3.8+**

| **Esconderse** | 0 cuadros/T | Se oculta en su posiciÃ³n (invisible) |- Type hints

| **Atacar** | 2 cuadros/T | Sprint final hacia el impala |- Docstrings formato Google

- PEP 8

## ğŸ“Š Archivos Guardados

## ğŸ“ Estado del Proyecto

Cada modelo entrenado genera 3 archivos en `modelos/`:

- [x] Arquitectura modular definida

1. **`nombre_conocimiento.json`**: Tabla Q completa- [x] Instrucciones para Copilot

2. **`nombre_config.json`**: ConfiguraciÃ³n (RADIO, parÃ¡metros de aprendizaje)- [x] MÃ³dulo de entorno

3. **`nombre_reporte.txt`**: MÃ©tricas y estadÃ­sticas del entrenamiento- [x] Agentes bÃ¡sicos (leÃ³n e impala)

- [x] Sistema de tiempo

### Formato de ConfiguraciÃ³n- [x] Verificador de condiciones

- [ ] MÃ³dulo de cacerÃ­a completo

```json- [ ] Sistema de conocimiento

{- [ ] Aprendizaje Q-Learning

  "radio": 9.5,- [ ] Interfaz de usuario

  "angulo_vision": 120,- [ ] Persistencia

  "distancia_minima_huida": 3,- [ ] Pruebas unitarias

  "cacerias_exitosas": 10450,- [ ] DocumentaciÃ³n completa

  "total_cacerias": 100000,

  "experiencias_unicas": 145135---

}

```**PrÃ³ximo paso**: Implementar `simulation/caceria.py` para orquestar una incursiÃ³n completa.


### VerificaciÃ³n de Compatibilidad

El sistema verifica que el **RADIO** del modelo coincida con el entorno actual:
- âœ… RADIO = 9.5 â†’ Compatible
- âŒ RADIO = 5.0 â†’ Incompatible (muestra advertencia)

## ğŸ”¬ Detalles TÃ©cnicos

### Estados del Mundo

Un estado se define por:
- **posicion_leon**: (r, Î¸) - Coordenadas polares
- **distancia_impala**: Distancia leÃ³n-impala (redondeada)
- **accion_impala**: AcciÃ³n actual del impala
- **leon_escondido**: Boolean (visible/invisible)
- **impala_puede_ver**: Boolean (dentro/fuera de visiÃ³n)

### Tabla Q

Estructura: `Dict[Estado, Dict[Accion, float]]`
- **Claves**: Estados del mundo (dataclass hashable)
- **Valores**: Diccionario de valores Q por acciÃ³n
- **TamaÃ±o tÃ­pico**: 145,000-675,000 experiencias Ãºnicas

### PolÃ­tica de ExploraciÃ³n

**Epsilon-greedy decreciente:**
- Inicio: Îµ = 1.0 (100% exploraciÃ³n)
- Decremento: Îµ -= 0.9/num_episodios por episodio
- Final: Îµ = 0.1 (10% exploraciÃ³n, 90% explotaciÃ³n)

## ğŸ† Resultados de Ejemplo

### Entrenamiento EM4 (100,000 episodios)

```
Total de cacerÃ­as: 100,000
CacerÃ­as exitosas: 10,450
Tasa de Ã©xito: 10.45%
Experiencias Ãºnicas: 145,135
Tiempo de entrenamiento: ~15 minutos
```

### ProgresiÃ³n TÃ­pica

| Episodios | Tasa de Ã‰xito |
|-----------|---------------|
| 1,000 | 3-4% |
| 10,000 | 6-8% |
| 50,000 | 9-10% |
| 100,000 | 10-12% |
| 500,000 | 12-15% |

## ğŸ› SoluciÃ³n de Problemas

### Error: "El RADIO del modelo no coincide"

El modelo fue entrenado con RADIO=5.0 (obsoleto). Entrenar nuevo modelo con RADIO=9.5.

### El impala siempre escapa

Normal con pocos episodios de entrenamiento. Entrenar con 100,000+ episodios.

### VisualizaciÃ³n no muestra colores

Terminal no soporta ANSI. Los emojis y ASCII seguirÃ¡n funcionando.

### Error de importaciÃ³n

Verificar que estÃ¡s ejecutando desde el directorio raÃ­z del proyecto.

## ğŸ“š Referencias

- **Q-Learning**: Watkins, C.J.C.H. (1989). Learning from Delayed Rewards
- **Aprendizaje por Refuerzo**: Sutton & Barto (2018). Reinforcement Learning: An Introduction

## ğŸ‘¨â€ğŸ’» Desarrollo

### PrÃ³ximas Mejoras

- [ ] Interfaz grÃ¡fica con pygame/tkinter
- [ ] MÃºltiples impalas en el abrevadero
- [ ] Terreno con obstÃ¡culos
- [ ] Deep Q-Networks (DQN)
- [ ] Algoritmos alternativos (SARSA, Actor-Critic)

### Contribuir

El proyecto fue desarrollado como trabajo final de Sistemas Inteligentes. Las contribuciones son bienvenidas.

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto para fines educativos.

---

**Desarrollado con:** Python 3.13 | Q-Learning | Aprendizaje por Refuerzo

**Estado:** âœ… Funcional - 10.45% tasa de Ã©xito con 100K episodios
