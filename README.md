# ü¶Å Le√≥n vs Impala - Q-Learning

Sistema de aprendizaje por refuerzo donde un le√≥n aprende a cazar un impala mediante **Q-Learning**. El le√≥n no tiene estrategia programada, sino que aprende por experiencia tras miles de cacer√≠as simuladas.

[![Python3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License:MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## üéØ Caracter√≠sticas

- ‚úÖ **Q-Learning** con exploraci√≥n epsilon-greedy
- ‚úÖ **Visualizaci√≥n ASCII** en terminal (Grid 19√ó19)
- ‚úÖ **Coordenadas polares** para movimiento natural
- ‚úÖ **Persistencia** de modelos entrenados
- ‚úÖ **Tests unitarios** completos (9/9 pasando)

## üöÄ Inicio R√°pido

\`\`\`bash
# Clonar repositorio
git clone https://github.com/Andark11/LeonvsImapala.git
cd LeonvsImapala

# Ejecutar
python main.py
\`\`\`

**Requisitos:** Python 3.8+ (sin dependencias externas)

## üìñ Uso

### 1. Entrenar un Le√≥n

\`\`\`bash
python main.py
# Seleccionar: 1 (Sistema de Entrenamiento)
# Episodios recomendados: 100,000
# Guardar modelo con nombre descriptivo
\`\`\`

**Resultados t√≠picos:**
- 10,000 episodios ‚Üí 6-8% √©xito
- 100,000 episodios ‚Üí 10-12% √©xito

### 2. Visualizar Cacer√≠a

\`\`\`bash
python main.py
# Seleccionar: 2 (Simulaci√≥n Visual)
# Elegir modelo entrenado
# Ver cacer√≠a en grid 19√ó19
\`\`\`

### 3. Ejecutar Tests

\`\`\`bash
python tests/test_basico.py
# Resultado: 9/9 tests pasando ‚úì
\`\`\`

## üß† Q-Learning Explicado

### ¬øQu√© es el Modelo Q-Learning?

**Q-Learning** es un algoritmo de **Aprendizaje por Refuerzo** (Reinforcement Learning) que permite a un agente (el le√≥n) aprender la mejor acci√≥n a tomar en cada situaci√≥n sin necesidad de un modelo expl√≠cito del entorno.

#### Concepto del Modelo

El modelo consiste en una **Tabla Q** que almacena valores Q(s,a) para cada combinaci√≥n de:
- **Estado (s)**: Situaci√≥n actual del mundo (posici√≥n le√≥n, distancia impala, visibilidad, etc.)
- **Acci√≥n (a)**: Movimiento posible (avanzar, esconderse, atacar)

**Valor Q(s,a)**: Representa "qu√© tan bueno" es tomar la acci√≥n `a` en el estado `s`. Un valor alto indica que hist√≥ricamente esa acci√≥n ha llevado a buenos resultados.

#### Tabla Q - Estructura

\`\`\`python
# Ejemplo de Tabla Q despu√©s de 1000 episodios
Q = {
    Estado(pos=1, dist=9.5, impala_bebe=True, escondido=False): {
        'avanzar': 45.2,      # Buena opci√≥n
        'esconderse': 58.7,   # Mejor opci√≥n (valor m√°s alto)
        'atacar': -15.3       # Mala opci√≥n (demasiado lejos)
    },
    Estado(pos=1, dist=2.0, impala_bebe=True, escondido=True): {
        'avanzar': 35.8,
        'esconderse': 12.1,
        'atacar': 78.5        # Mejor opci√≥n (cerca y escondido)
    }
}
\`\`\`

### La Ecuaci√≥n de Bellman

El le√≥n aprende actualizando los valores Q mediante la **ecuaci√≥n de Bellman**:

\`\`\`
Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥¬∑max Q(s',a') - Q(s,a)]
\`\`\`

**Desglose de la ecuaci√≥n:**

1. **Q(s,a)**: Valor Q actual que queremos actualizar
2. **Œ± (alpha)** = 0.05: **Tasa de aprendizaje**
   - Controla qu√© tan r√°pido se actualizan los valores
   - Valor bajo (0.05) = aprendizaje gradual y estable
   - Evita cambios bruscos por experiencias aisladas

3. **r**: **Recompensa inmediata** obtenida
   - +100 puntos: Cacer√≠a exitosa (captur√≥ al impala)
   - -50 puntos: Fracaso (impala escap√≥)
   - +1 punto: Por cada cuadro que se acerc√≥
   - -10 puntos: Si el impala detecta al le√≥n prematuramente

4. **Œ≥ (gamma)** = 0.9: **Factor de descuento**
   - Importancia de recompensas futuras vs inmediatas
   - 0.9 = valora mucho las consecuencias a largo plazo
   - M√°s cercano a 1 = m√°s "visionario"

5. **max Q(s',a')**: **Mejor valor Q futuro**
   - Mejor acci√≥n posible en el nuevo estado s'
   - Estimaci√≥n del valor futuro √≥ptimo
   - Gu√≠a hacia decisiones que maximizan recompensa total

6. **[r + Œ≥¬∑max Q(s',a') - Q(s,a)]**: **Error de predicci√≥n**
   - Diferencia entre lo esperado y lo obtenido
   - Si es positivo: la acci√≥n fue mejor de lo esperado
   - Si es negativo: fue peor de lo esperado

#### Interpretaci√≥n Intuitiva

La ecuaci√≥n dice: *"El valor de tomar la acci√≥n A en el estado S es mi estimaci√≥n actual m√°s un ajuste basado en lo que realmente pas√≥ (recompensa inmediata + mejor futuro posible)"*

### Ejemplo de Aprendizaje

**Episodio 1** (sin experiencia):
\`\`\`
Estado: Le√≥n en pos 1, distancia 9.5, impala bebiendo
Q inicial: Q(estado, atacar) = 0

Le√≥n toma acci√≥n: ATACAR (aleatorio, no sabe que es malo)
Resultado: Impala detecta el sonido y escapa
Recompensa: r = -50

Actualizaci√≥n:
Q(estado, atacar) = 0 + 0.05[-50 + 0 - 0]
Q(estado, atacar) = -2.5

üß† Aprendi√≥: "Atacar desde lejos es muy mala idea"
\`\`\`

**Episodio 100** (con experiencia):
\`\`\`
Mismo estado: Le√≥n en pos 1, distancia 9.5, impala bebiendo
Q actual: Q(estado, esconderse) = 45.0 (mejor opci√≥n conocida)

Le√≥n toma acci√≥n: ESCONDERSE (explota conocimiento)
Resultado: No es detectado, puede acercarse despu√©s
Recompensa: r = +5 (bono por estrategia)

Actualizaci√≥n:
Q(estado, esconderse) = 45 + 0.05[5 + 0.9(50) - 45]
Q(estado, esconderse) = 45 + 0.05[5 + 45 - 45]
Q(estado, esconderse) = 45.25

üß† Reforz√≥: "Esconderse desde lejos funciona bien"
\`\`\`

### Proceso de Aprendizaje

1. **Exploraci√≥n** ‚Üí Prueba acciones aleatorias para descubrir
2. **Experiencia** ‚Üí Acumula resultados (estado ‚Üí acci√≥n ‚Üí recompensa ‚Üí nuevo estado)
3. **Actualizaci√≥n** ‚Üí Mejora valores Q con la ecuaci√≥n de Bellman
4. **Explotaci√≥n** ‚Üí Usa conocimiento aprendido (elige acciones con Q alto)
5. **Convergencia** ‚Üí Despu√©s de miles de episodios, desarrolla estrategia √≥ptima

### Pol√≠tica Epsilon-Greedy

Balancea **exploraci√≥n** (descubrir) vs **explotaci√≥n** (usar lo aprendido):

\`\`\`python
if random() < epsilon:
    acci√≥n = aleatoria()      # EXPLORAR: probar algo nuevo
else:
    acci√≥n = argmax(Q[estado])  # EXPLOTAR: mejor acci√≥n conocida
\`\`\`

**Decaimiento de epsilon:**
- Inicio: Œµ = 1.0 (100% exploraci√≥n - el le√≥n no sabe nada)
- Decremento: Œµ -= 0.9/episodios_totales (decrece gradualmente)
- Final: Œµ = 0.1 (90% explotaci√≥n, 10% exploraci√≥n - el le√≥n usa su experiencia pero sigue probando cosas nuevas ocasionalmente)

## üåç Coordenadas Polares

### ¬øQu√© son las Coordenadas Polares?

En lugar de usar coordenadas cartesianas (x, y), las coordenadas polares definen un punto mediante:
- **r (radio)**: Distancia desde el centro (el abrevadero)
- **Œ∏ (theta)**: √Ångulo desde el norte (0¬∞ = Norte, aumenta en sentido horario)

### Diagrama del Sistema

\`\`\`
        N (0¬∞)
         |
    8    1    2
     \   |   /
315¬∞ \  0¬∞  / 45¬∞
      \ | /
  W -- AB -- E
      / | \
270¬∞ /  |  \ 90¬∞
    /   |   \
    7   6   5
        |
       S (180¬∞)
        
Posiciones:
1 = Norte (N)      - 0¬∞
2 = Noreste (NE)   - 45¬∞
3 = Este (E)       - 90¬∞
4 = Sureste (SE)   - 135¬∞
5 = Sur (S)        - 180¬∞
6 = Suroeste (SO)  - 225¬∞
7 = Oeste (O)      - 270¬∞
8 = Noroeste (NO)  - 315¬∞
AB = Abrevadero    - Centro (0, 0)
\`\`\`

### C√°lculo de √Ångulo desde Posici√≥n

**F√≥rmula:**
\`\`\`python
Œ∏ = (posici√≥n - 1) √ó 45¬∞
\`\`\`

**Ejemplos:**
\`\`\`
posici√≥n 1 (Norte):     Œ∏ = (1-1) √ó 45¬∞ = 0¬∞
posici√≥n 2 (Noreste):   Œ∏ = (2-1) √ó 45¬∞ = 45¬∞
posici√≥n 3 (Este):      Œ∏ = (3-1) √ó 45¬∞ = 90¬∞
posici√≥n 5 (Sur):       Œ∏ = (5-1) √ó 45¬∞ = 180¬∞
posici√≥n 7 (Oeste):     Œ∏ = (7-1) √ó 45¬∞ = 270¬∞
posici√≥n 8 (Noroeste):  Œ∏ = (8-1) √ó 45¬∞ = 315¬∞
\`\`\`

### Conversi√≥n Polar ‚Üí Cartesiano

Para convertir coordenadas polares (r, Œ∏) a coordenadas cartesianas (x, y):

**F√≥rmulas:**
\`\`\`python
x = r √ó sin(Œ∏)
y = r √ó cos(Œ∏)
\`\`\`

**Ejemplos con RADIO = 9.5:**

#### Posici√≥n 1 (Norte, Œ∏=0¬∞):
\`\`\`python
x = 9.5 √ó sin(0¬∞) = 9.5 √ó 0 = 0.0
y = 9.5 √ó cos(0¬∞) = 9.5 √ó 1 = 9.5
‚Üí Coordenadas: (0.0, 9.5)
\`\`\`

#### Posici√≥n 2 (Noreste, Œ∏=45¬∞):
\`\`\`python
x = 9.5 √ó sin(45¬∞) = 9.5 √ó 0.707 = 6.72
y = 9.5 √ó cos(45¬∞) = 9.5 √ó 0.707 = 6.72
‚Üí Coordenadas: (6.72, 6.72)
\`\`\`

#### Posici√≥n 3 (Este, Œ∏=90¬∞):
\`\`\`python
x = 9.5 √ó sin(90¬∞) = 9.5 √ó 1 = 9.5
y = 9.5 √ó cos(90¬∞) = 9.5 √ó 0 = 0.0
‚Üí Coordenadas: (9.5, 0.0)
\`\`\`

#### Posici√≥n 5 (Sur, Œ∏=180¬∞):
\`\`\`python
x = 9.5 √ó sin(180¬∞) = 9.5 √ó 0 = 0.0
y = 9.5 √ó cos(180¬∞) = 9.5 √ó (-1) = -9.5
‚Üí Coordenadas: (0.0, -9.5)
\`\`\`

#### Posici√≥n 7 (Oeste, Œ∏=270¬∞):
\`\`\`python
x = 9.5 √ó sin(270¬∞) = 9.5 √ó (-1) = -9.5
y = 9.5 √ó cos(270¬∞) = 9.5 √ó 0 = 0.0
‚Üí Coordenadas: (-9.5, 0.0)
\`\`\`

### C√°lculo de Distancia

Para calcular la distancia entre dos puntos en coordenadas cartesianas:

**F√≥rmula de distancia euclidiana:**
\`\`\`python
d = ‚àö[(x‚ÇÇ - x‚ÇÅ)¬≤ + (y‚ÇÇ - y‚ÇÅ)¬≤]
\`\`\`

#### Ejemplo: Distancia desde Le√≥n (pos 1) hasta Abrevadero

\`\`\`python
# Le√≥n en posici√≥n 1 (Norte)
le√≥n_x = 0.0
le√≥n_y = 9.5

# Abrevadero en el centro
abrevadero_x = 0.0
abrevadero_y = 0.0

# Distancia
d = ‚àö[(0.0 - 0.0)¬≤ + (0.0 - 9.5)¬≤]
d = ‚àö[0 + 90.25]
d = ‚àö90.25
d = 9.5 unidades ‚úì
\`\`\`

#### Ejemplo: Distancia entre Le√≥n (pos 1) e Impala (pos 5)

\`\`\`python
# Le√≥n en posici√≥n 1 (Norte): (0.0, 9.5)
# Impala en posici√≥n 5 (Sur): (0.0, -9.5)

d = ‚àö[(0.0 - 0.0)¬≤ + (-9.5 - 9.5)¬≤]
d = ‚àö[0 + (-19)¬≤]
d = ‚àö361
d = 19.0 unidades (di√°metro completo)
\`\`\`

### Conversi√≥n a Grid ASCII

Para visualizaci√≥n en terminal, se convierte a un grid 19√ó19:

**F√≥rmula:**
\`\`\`python
ESCALA = 1.9
grid_x = int(x_cartesiano * ESCALA) + 9  # +9 para centrar (grid 0-18)
grid_y = int(y_cartesiano * ESCALA) + 9
\`\`\`

**Ejemplo - Le√≥n en posici√≥n 1:**
\`\`\`python
# Coordenadas cartesianas: (0.0, 9.5)
grid_x = int(0.0 √ó 1.9) + 9 = 0 + 9 = 9
grid_y = int(9.5 √ó 1.9) + 9 = 18 + 9 = 27 ‚Üí ajustado a 18 (l√≠mite grid)

# En el grid ASCII, el le√≥n aparece en columna 9, fila superior
\`\`\`

### Ventajas del Sistema Polar

- **Natural para escenario circular**: El abrevadero es el centro natural
- **Simplifica c√°lculos de distancia**: Solo necesitamos el radio
- **Movimiento intuitivo**: Avanzar = reducir r (acercarse al centro)
- **8 direcciones claras**: Posiciones cardinales f√°ciles de entender

### Par√°metros del Sistema

- **RADIO** = 9.5 unidades (distancia inicial le√≥n-impala desde el abrevadero)
- **ESCALA** = 1.9 (factor de conversi√≥n a grid ASCII 19√ó19)
- **Posiciones**: 8 puntos cardinales + 1 centro (abrevadero)
- **Rango √°ngulos**: 0¬∞ a 315¬∞ (incrementos de 45¬∞)

## üìÅ Estructura

\`\`\`
LeonvsImapala/
‚îú‚îÄ‚îÄ agents/          # Le√≥n e Impala
‚îú‚îÄ‚îÄ simulation/      # Motor de cacer√≠a
‚îú‚îÄ‚îÄ knowledge/       # Base de conocimientos
‚îú‚îÄ‚îÄ learning/        # Q-Learning y entrenamiento
‚îú‚îÄ‚îÄ storage/         # Persistencia JSON
‚îú‚îÄ‚îÄ ui/              # Interfaces (terminal + matplotlib)
‚îú‚îÄ‚îÄ tests/           # Tests unitarios
‚îú‚îÄ‚îÄ modelos/         # Modelos entrenados (generados)
‚îú‚îÄ‚îÄ environment.py   # Abrevadero y coordenadas
‚îî‚îÄ‚îÄ main.py          # Punto de entrada
\`\`\`

## üéÆ Reglas del Sistema

### Entorno
- **Grid:** 19√ó19 cuadros
- **RADIO:** 9.5 unidades (distancia inicial le√≥n-impala)
- **Captura:** Distancia ‚â§ 0.5 unidades

### Acciones del Le√≥n
| Acci√≥n | Velocidad | Descripci√≥n |
|--------|-----------|-------------|
| Avanzar | 1 cuadro/T | Acercarse sigilosamente |
| Esconderse | 0 cuadros/T | Ocultarse (invisible) |
| Atacar | 2 cuadros/T | Sprint final |

### Comportamiento del Impala
- **Visi√≥n:** Cono de 120¬∞ (puede rotar)
- **Huida:** Aceleraci√≥n progresiva (1‚Üí2‚Üí3‚Üí4... cuadros/T)
- **Condiciones de huida:**
  1. Ve al le√≥n (no escondido)
  2. Le√≥n ataca
  3. Distancia < 3 cuadros

## üß™ Tests Unitarios

\`\`\`bash
‚úì Abrevadero - Coordenadas (RADIO=9.5)
‚úì Abrevadero - Distancias
‚úì Le√≥n - Acciones (avanzar, esconderse, atacar)
‚úì Impala - Acciones (ver, beber, huir)
‚úì Base Conocimientos - Tabla Q
‚úì Q-Learning - Selecci√≥n epsilon-greedy
‚úì Sistema Recompensas
‚úì Cacer√≠a Completa - End-to-end
‚úì Cacer√≠a Turno a Turno
\`\`\`

**Cobertura:** Entorno, agentes, aprendizaje, simulaci√≥n, conocimiento

## üìä Resultados

### Modelo EM4 (100,000 episodios)
\`\`\`
Total de cacer√≠as: 100,000
Cacer√≠as exitosas: 10,450
Tasa de √©xito: 10.45%
Experiencias √∫nicas: 145,135
Tiempo: ~15 minutos
\`\`\`

### Progresi√≥n T√≠pica
| Episodios | Tasa √âxito |
|-----------|------------|
| 1,000 | 3-4% |
| 10,000 | 6-8% |
| 50,000 | 9-10% |
| 100,000 | 10-12% |

## üîß Tecnolog√≠a

- **Python 3.8+** con type hints
- **Q-Learning** (Reinforcement Learning)
- **Sin dependencias** (solo stdlib)
- **JSON** para persistencia
- **ASCII art** para visualizaci√≥n

## üêõ Troubleshooting

**Error: "El RADIO del modelo no coincide"**
‚Üí Re-entrenar con RADIO=9.5

**El impala siempre escapa**
‚Üí Normal con pocos episodios. Entrenar 100K+

**No se ven colores en terminal**
‚Üí Terminal no soporta ANSI, pero funciona igual

## üìö Referencias

- Watkins, C.J.C.H. (1989). *Learning from Delayed Rewards*
- Sutton & Barto (2018). *Reinforcement Learning: An Introduction*

## üìÑ Licencia

MIT License - Ver archivo LICENSE para detalles

## üë®‚Äçüíª Autor

**Proyecto Final - Sistemas Inteligentes**  
Implementaci√≥n educativa de Q-Learning aplicado a caza predador-presa

---

**Estado:** ‚úÖ Sistema completo y funcional  
**Versi√≥n:** 1.0.0  
**√öltima actualizaci√≥n:** Diciembre 2025
