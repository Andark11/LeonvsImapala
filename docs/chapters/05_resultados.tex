\chapter{Resultados y Análisis}

\section{Métricas de Desempeño}

\subsection{Modelo EM4 (100,000 episodios)}

\begin{table}[H]
\centering
\begin{tabular}{lr}
\toprule
\textbf{Métrica} & \textbf{Valor} \\
\midrule
Episodios totales & 100,000 \\
Cacerías exitosas & 10,450 \\
Cacerías fallidas & 89,550 \\
Tasa de éxito & 10.45\% \\
Experiencias únicas & 145,135 \\
Tiempo de entrenamiento & $\sim$15 minutos \\
Recompensa promedio & +12.4 \\
Tamaño modelo (JSON) & 24.3 MB \\
\bottomrule
\end{tabular}
\caption{Métricas del modelo EM4 entrenado}
\label{tab:metricas_em4}
\end{table}

\subsection{Progresión del Aprendizaje}

\begin{table}[H]
\centering
\begin{tabular}{rrrr}
\toprule
\textbf{Episodios} & \textbf{Tasa Éxito} & \textbf{Epsilon} & \textbf{Comportamiento} \\
\midrule
1,000 & 3.2\% & 0.991 & Caótico, explora aleatoriamente \\
10,000 & 6.8\% & 0.910 & Identifica patrones básicos \\
30,000 & 8.5\% & 0.730 & Consolida estrategias \\
50,000 & 9.7\% & 0.550 & Refina decisiones \\
75,000 & 10.2\% & 0.325 & Cerca de óptimo \\
100,000 & 10.5\% & 0.100 & Estrategia estable \\
\bottomrule
\end{tabular}
\caption{Evolución del desempeño durante entrenamiento}
\label{tab:evolucion}
\end{table}

\begin{figure}[H]
\centering
\begin{verbatim}
Tasa de Éxito (%)
    │
12  ├─────────────────────────────────────────┐
    │                                         ◊
11  │                                    ◊ ◊
    │                              ◊ ◊
10  │                         ◊ ◊
    │                    ◊ ◊
 9  │               ◊ ◊
    │          ◊ ◊
 8  │      ◊ ◊
    │   ◊ ◊
 7  │  ◊
    │ ◊
 6  ├◊
    │
 5  └┴────┴────┴────┴────┴────┴────┴────┴────┴─→
    0   10K  20K  30K  40K  50K  60K  70K  80K  100K
                        Episodios
\end{verbatim}
\caption{Curva de aprendizaje del león (ASCII)}
\label{fig:curva_aprendizaje}
\end{figure}

\section{Estrategias Emergentes}

\subsection{Las 5 Reglas Aprendidas}

Después de 100,000 episodios, el león desarrolló naturalmente estas estrategias:

\begin{enumerate}
    \item \textbf{Esconderse primero}: Desde posiciones iniciales lejanas ($d > 7$), siempre se esconde antes de avanzar. Evita detección temprana con penalización de hasta -10.0 puntos.
    
    \item \textbf{Avanzar oculto}: Mientras está invisible, maximiza el acercamiento acumulando +1.0 por cuadro sin riesgo de detección.
    
    \item \textbf{Atacar cerca}: Solo inicia ataque cuando $d < 2$ cuadros. Diferencia de 8 puntos entre atacar cerca (+5.0) vs lejos (-3.0).
    
    \item \textbf{Timing perfecto}: Prioriza atacar cuando el impala bebe agua (ciego y vulnerable). Incrementa tasa de éxito en 15-20\%.
    
    \item \textbf{No perseguir}: Si es detectado a distancia $> 4$ cuadros, el león aprendió que perseguir es inútil (impala acelera y escapa). Mejor reintentar en siguiente episodio.
\end{enumerate}

\subsection{Análisis de Decisiones por Distancia}

\begin{table}[H]
\centering
\begin{tabular}{lrrr}
\toprule
\textbf{Rango Distancia} & \textbf{Acción} & \textbf{Frecuencia} & \textbf{Valor Q Promedio} \\
\midrule
$d > 7$ cuadros & ESCONDERSE & 78\% & +45.2 \\
                & AVANZAR & 15\% & +12.3 \\
                & ATACAR & 7\% & -25.1 \\
\midrule
$4 < d \leq 7$ & AVANZAR & 65\% & +32.5 \\
               & ESCONDERSE & 30\% & +28.7 \\
               & ATACAR & 5\% & -8.4 \\
\midrule
$2 < d \leq 4$ & AVANZAR & 55\% & +38.9 \\
               & ATACAR & 35\% & +15.2 \\
               & ESCONDERSE & 10\% & +8.1 \\
\midrule
$d \leq 2$ & ATACAR & 85\% & +68.5 \\
           & AVANZAR & 10\% & +22.3 \\
           & ESCONDERSE & 5\% & -5.2 \\
\bottomrule
\end{tabular}
\caption{Distribución de acciones por distancia (modelo EM4)}
\label{tab:acciones_distancia}
\end{table}

\section{Comparación Cacería Exitosa vs Fallida}

\subsection{Cacería Exitosa Típica}

\begin{table}[H]
\centering
\begin{tabular}{clr}
\toprule
\textbf{Turno} & \textbf{Acción} & \textbf{Recompensa} \\
\midrule
1 & ESCONDERSE (preparación) & -1.1 \\
2 & AVANZAR (oculto, 9.5 $\to$ 8.5) & +0.9 \\
3 & AVANZAR (oculto, 8.5 $\to$ 7.5) & +0.9 \\
4 & AVANZAR (oculto, 7.5 $\to$ 6.5) & +0.9 \\
5 & AVANZAR (oculto, 6.5 $\to$ 5.5) & +0.9 \\
6 & AVANZAR (oculto, 5.5 $\to$ 4.5) & +0.9 \\
7 & AVANZAR (oculto, 4.5 $\to$ 3.5) & +0.9 \\
8 & ATACAR (3.5 $\to$ 0.0, CAPTURA) & +106.4 \\
\midrule
\multicolumn{2}{l}{\textbf{Total}} & \textbf{+111.1} \\
\bottomrule
\end{tabular}
\caption{Desglose de cacería exitosa}
\label{tab:caceria_exitosa}
\end{table}

\subsection{Cacería Fallida Típica}

\begin{table}[H]
\centering
\begin{tabular}{clr}
\toprule
\textbf{Turno} & \textbf{Acción} & \textbf{Recompensa} \\
\midrule
1 & AVANZAR visible (DETECTADO) & -9.1 \\
2 & AVANZAR (persecución inútil) & -0.1 \\
3 & ATACAR lejos (prematuro) & -5.1 \\
4 & Perseguir (impala más rápido) & -6.1 \\
5 & Perseguir (impala más rápido) & -6.1 \\
6 & Perseguir (impala más rápido) & -6.1 \\
7 & Perseguir (impala más rápido) & -6.1 \\
8 & ESCAPE del impala (fracaso) & -68.1 \\
\midrule
\multicolumn{2}{l}{\textbf{Total}} & \textbf{-88.5} \\
\bottomrule
\end{tabular}
\caption{Desglose de cacería fallida}
\label{tab:caceria_fallida}
\end{table}

\subsection{Diferencia Estratégica}

\begin{equation}
\Delta R = R_{\text{éxito}} - R_{\text{fracaso}} = 111.1 - (-88.5) = 199.6 \text{ puntos}
\end{equation}

Esta diferencia masiva refuerza el aprendizaje hacia estrategias óptimas.

\section{Análisis de la Base de Conocimientos}

\subsection{Patrones Identificados}

El sistema de generalización identificó 47 patrones recurrentes:

\begin{table}[H]
\centering
\begin{tabular}{p{6cm}rr}
\toprule
\textbf{Patrón} & \textbf{Éxitos} & \textbf{Tasa} \\
\midrule
Esconderse + Avanzar oculto + Atacar cerca & 8,234 & 78.8\% \\
Avanzar visible desde lejos + Detectado & 1,245 & 2.1\% \\
Atacar sin esconderse primero & 892 & 5.3\% \\
Esconderse cuando impala mira otro lado & 356 & 8.9\% \\
Atacar cuando impala bebe agua & 1,678 & 85.2\% \\
\bottomrule
\end{tabular}
\caption{Top 5 patrones más frecuentes}
\label{tab:patrones}
\end{table}

\subsection{Generalización Efectiva}

El sistema de generalización permitió transferir conocimiento a situaciones nuevas:

\begin{itemize}
    \item \textbf{Cobertura de estados}: 145,135 estados únicos visitados
    \item \textbf{Estados similares}: ~892,000 situaciones resueltas por similitud
    \item \textbf{Factor de amplificación}: 6.14× (892K / 145K)
\end{itemize}

\section{Análisis Estadístico}

\subsection{Distribución de Duraciones}

\begin{table}[H]
\centering
\begin{tabular}{lrr}
\toprule
\textbf{Duración (turnos)} & \textbf{Frecuencia} & \textbf{Resultado} \\
\midrule
1-5 & 23,456 & 85\% éxito, 15\% fracaso \\
6-10 & 45,123 & 12\% éxito, 88\% fracaso \\
11-20 & 28,891 & 2\% éxito, 98\% fracaso \\
21-50 & 2,530 & 0\% éxito, 100\% fracaso \\
\bottomrule
\end{tabular}
\caption{Distribución de duraciones de cacerías}
\label{tab:duraciones}
\end{table}

\textbf{Conclusión}: Cacerías exitosas son típicamente cortas (< 10 turnos). Cacerías largas casi siempre fallan.

\subsection{Impacto de Posición Inicial}

\begin{table}[H]
\centering
\begin{tabular}{clr}
\toprule
\textbf{Posición} & \textbf{Dirección} & \textbf{Tasa Éxito} \\
\midrule
1 & Norte & 11.2\% \\
2 & Noreste & 12.5\% \\
3 & Este & 10.8\% \\
4 & Sureste & 12.1\% \\
5 & Sur & 9.8\% \\
6 & Suroeste & 11.9\% \\
7 & Oeste & 10.3\% \\
8 & Noroeste & 12.4\% \\
\midrule
\multicolumn{2}{l}{\textbf{Promedio}} & \textbf{11.0\%} \\
\bottomrule
\end{tabular}
\caption{Tasa de éxito por posición inicial}
\label{tab:posicion_exito}
\end{table}

\textbf{Observación}: Posiciones diagonales (2, 4, 6, 8) tienen ligeramente mejor desempeño (+1.5\% promedio) que posiciones cardinales (1, 3, 5, 7).

\section{Validación Experimental}

\subsection{Tests de Robustez}

\begin{enumerate}
    \item \textbf{Reproducibilidad}: 5 entrenamientos independientes alcanzaron 10.2-10.8\% de éxito (desviación estándar: 0.24\%).
    
    \item \textbf{Sensibilidad a $\alpha$}: Probamos $\alpha \in \{0.01, 0.05, 0.1, 0.2\}$. Óptimo: $\alpha = 0.05$.
    
    \item \textbf{Sensibilidad a $\gamma$}: Probamos $\gamma \in \{0.7, 0.8, 0.9, 0.95\}$. Óptimo: $\gamma = 0.9$.
    
    \item \textbf{Convergencia}: Valores Q se estabilizan después de ~75,000 episodios.
\end{enumerate}

\subsection{Comparación con Baseline}

\begin{table}[H]
\centering
\begin{tabular}{lrr}
\toprule
\textbf{Estrategia} & \textbf{Tasa Éxito} & \textbf{Recompensa Promedio} \\
\midrule
Aleatoria & 1.2\% & -45.3 \\
Greedy simple & 4.8\% & -12.7 \\
Q-Learning (10K eps) & 6.8\% & +2.4 \\
Q-Learning (100K eps) & 10.5\% & +12.4 \\
\bottomrule
\end{tabular}
\caption{Comparación de estrategias}
\label{tab:comparacion}
\end{table}

\textbf{Conclusión}: Q-Learning supera significativamente baselines simples.

\section{Limitaciones Observadas}

\subsection{Techo de Desempeño}

La tasa de éxito máxima observada es ~12\% después de 200,000 episodios. Esto se debe a:

\begin{enumerate}
    \item Ventajas naturales del impala (visión, aceleración)
    \item Estocasticidad en comportamiento del impala
    \item Límite inherente del escenario adversarial
\end{enumerate}

\subsection{Tiempo de Entrenamiento}

Mientras 100,000 episodios toman ~15 minutos, entrenamientos más largos muestran rendimientos decrecientes:

\begin{itemize}
    \item 100K $\to$ 200K: +1.2\% mejora (2 horas adicionales)
    \item 200K $\to$ 500K: +0.3\% mejora (8 horas adicionales)
\end{itemize}

\section{Interpretabilidad}

Una ventaja clave de Q-Learning tabular es la interpretabilidad. Podemos inspeccionar valores Q directamente:

\begin{lstlisting}[caption=Valores Q para estado específico]
Estado: (pos=1, dist=9.5, escondido=False, impala_bebe=True)

Q-values:
  avanzar:     +12.34
  esconderse:  +58.71  ← MEJOR ACCIÓN
  atacar:      -15.32
  
Interpretación: En este estado, el león ha aprendido que
esconderse es la mejor opción (casi 5× mejor que avanzar).
\end{lstlisting}

Esta transparencia facilita debugging, análisis y confianza en el sistema.
